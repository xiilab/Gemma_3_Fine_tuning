# Gemma-2b μ½”λ“ νμΈνλ‹ with MLflow μ—°μ† ν•™μµ

μ΄ ν”„λ΅μ νΈλ” Google Gemma-2b λ¨λΈμ„ μ½”λ“ λ°μ΄ν„°λ΅ νμΈνλ‹ν•κ³ , MLflowλ¥Ό ν†µν•΄ λ¨λΈμ„ κ΄€λ¦¬ν•λ©° μ—°μ† ν•™μµμ„ μ§€μ›ν•©λ‹λ‹¤.

## π€ μ£Όμ” κΈ°λ¥

- **QLoRA νμΈνλ‹**: λ©”λ¨λ¦¬ ν¨μ¨μ μΈ LoRA κΈ°λ° νμΈνλ‹
- **MLflow ν†µν•©**: μ‹¤ν— μ¶”μ , λ¨λΈ λ²„μ „ κ΄€λ¦¬, μ•„ν‹°ν©νΈ μ €μ¥
- **μ—°μ† ν•™μµ**: μ΄μ „ ν•™μµλ λ¨λΈμ„ λ΅λ“ν•μ—¬ μ¶”κ°€ ν•™μµ κ°€λ¥
- **λ°μ΄ν„°μ…‹ λ²”μ„ μ„ νƒ**: `dataset_start`μ™€ `dataset_end`λ΅ νΉμ • λ²”μ„μ λ°μ΄ν„°λ§ ν•™μµ
- **μλ™ λ¨λΈ μ €μ¥**: ν•™μµ μ™„λ£ ν›„ μλ™μΌλ΅ MLflowμ— λ¨λΈ λ“±λ΅

## π“‹ μ‚¬μ© λ°©λ²•

### 1. μƒλ΅μ΄ λ¨λΈλ΅ ν•™μµ μ‹μ‘

```bash
python main.py
```

### 2. MLflowμ—μ„ κΈ°μ΅΄ λ¨λΈμ„ λ΅λ“ν•μ—¬ μ—°μ† ν•™μµ

#### μ‚¬μ© κ°€λ¥ν• λ¨λΈ λ©λ΅ ν™•μΈ
```bash
python continue_training.py --list-models
```

#### μµκ·Ό ν•™μµ μ‹¤ν–‰ λ©λ΅ ν™•μΈ
```bash
python continue_training.py --list-runs
```

#### νΉμ • λ¨λΈ μ΄λ¦„μΌλ΅ μ—°μ† ν•™μµ
```bash
python continue_training.py --model-name gemma-2b-code-finetuned --epochs 2 --learning-rate 1e-4
```

#### νΉμ • run_idλ΅ μ—°μ† ν•™μµ
```bash
python continue_training.py --run-id abc123def456 --epochs 1 --batch-size 4
```

#### νΉμ • λ°μ΄ν„°μ…‹ λ²”μ„λ΅ μ—°μ† ν•™μµ
```bash
# 5000~15000 λ²”μ„μ λ°μ΄ν„°λ΅ μ—°μ† ν•™μµ
python continue_training.py --model-name gemma-2b-code-finetuned --dataset-start 5000 --dataset-end 15000

# 10000~20000 λ²”μ„μ λ°μ΄ν„°λ΅ μ—°μ† ν•™μµ  
python continue_training.py --model-name gemma-2b-code-finetuned --dataset-start 10000 --dataset-end 20000 --epochs 2
```

### 3. ν•μ΄νΌνλΌλ―Έν„° μ§μ ‘ μμ •

`main.py` νμΌμ—μ„ `hyperparams` λ”•μ…”λ„λ¦¬λ¥Ό μμ •ν•μ—¬ μ—°μ† ν•™μµ μ„¤μ •:

```python
hyperparams = {
    # ... κΈ°μ΅΄ μ„¤μ • ...
    "continue_from_model": "gemma-2b-code-finetuned",  # MLflowμ—μ„ λ΅λ“ν•  λ¨λΈ μ΄λ¦„
    "continue_from_run_id": None,  # λλ” νΉμ • run_id
    "num_epochs": 2,  # μ¶”κ°€ ν•™μµν•  μ—ν¬ν¬ μ
    "learning_rate": 1e-4,  # μƒλ΅μ΄ ν•™μµλ¥ 
    "dataset_start": 5000,  # λ°μ΄ν„°μ…‹ μ‹μ‘ μΈλ±μ¤
    "dataset_end": 15000,   # λ°μ΄ν„°μ…‹ λ μΈλ±μ¤ (exclusive)
}
```

## π”§ μ„¤μ •

### MLflow μ„λ²„ μ„¤μ •
- **μ„λ²„ μ£Όμ†**: http://10.61.3.161:30744/
- **μ‹¤ν— μ΄λ¦„**: Gemma-2b-Code-Finetuning
- **λ¨λΈ λ μ§€μ¤νΈλ¦¬**: μλ™μΌλ΅ λ¨λΈμ΄ λ“±λ΅λ¨

### κΈ°λ³Έ ν•μ΄νΌνλΌλ―Έν„°
```python
{
    "model_name": "google/gemma-2b",
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "batch_size": 2,
    "learning_rate": 2e-4,
    "weight_decay": 0.01,
    "num_epochs": 1,
    "max_length": 512,
    "dataset_start": 0,  # λ°μ΄ν„°μ…‹ μ‹μ‘ μΈλ±μ¤
    "dataset_end": 10000,  # λ°μ΄ν„°μ…‹ λ μΈλ±μ¤ (exclusive)
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
}
```

## π“ MLflow UIμ—μ„ ν™•μΈ

ν•™μµ μ™„λ£ ν›„ MLflow UIμ—μ„ λ‹¤μμ„ ν™•μΈν•  μ μμµλ‹λ‹¤:

1. **μ‹¤ν— μ¶”μ **: http://10.61.3.161:30744/
2. **λ¨λΈ λ μ§€μ¤νΈλ¦¬**: λ“±λ΅λ λ¨λΈ λ²„μ „ κ΄€λ¦¬
3. **λ©”νΈλ¦­**: Loss, νλΌλ―Έν„° μ, ν•™μµ μ§„ν–‰ μƒν™©
4. **μ•„ν‹°ν©νΈ**: μ €μ¥λ λ¨λΈ νμΌ, μ„¤μ • νμΌ

## π”„ μ—°μ† ν•™μµ μ›ν¬ν”λ΅μ°

1. **μ΄κΈ° ν•™μµ**: `python main.py`λ΅ μ²« λ²μ§Έ ν•™μµ μ‹¤ν–‰
2. **λ¨λΈ ν™•μΈ**: MLflow UIμ—μ„ ν•™μµλ λ¨λΈ ν™•μΈ
3. **μ—°μ† ν•™μµ**: `continue_training.py`λ΅ μ¶”κ°€ ν•™μµ μ„¤μ •
   - λ¨λΈ μ„ νƒ: `--model-name` λλ” `--run-id`
   - λ°μ΄ν„°μ…‹ λ²”μ„: `--dataset-start`, `--dataset-end`
   - ν•™μµ νλΌλ―Έν„°: `--epochs`, `--learning-rate`, `--batch-size`
4. **μ¬μ‹¤ν–‰**: `python main.py`λ΅ μ—°μ† ν•™μµ μ‹¤ν–‰
5. **λ°λ³µ**: ν•„μ”μ— λ”°λΌ λ‹¨κ³„ 2-4 λ°λ³µ

## π“ νμΌ κµ¬μ΅°

```
.
β”β”€β”€ main.py                 # λ©”μΈ ν•™μµ μ¤ν¬λ¦½νΈ
β”β”€β”€ continue_training.py    # μ—°μ† ν•™μµ μ„¤μ • λ„κµ¬
β”β”€β”€ README.md              # μ΄ νμΌ
β””β”€β”€ requirements.txt       # μμ΅΄μ„± ν¨ν‚¤μ§€
```

## β οΈ μ£Όμμ‚¬ν•­

1. **λ©”λ¨λ¦¬ μ”κµ¬μ‚¬ν•­**: GPU λ©”λ¨λ¦¬κ°€ μ¶©λ¶„ν•μ§€ ν™•μΈ
2. **λ°μ΄ν„°μ…‹ κ²½λ΅**: `/datasets/github-code/` κ²½λ΅μ— λ°μ΄ν„°μ…‹μ΄ μμ–΄μ•Ό ν•¨
3. **MLflow μ—°κ²°**: λ„¤νΈμ›ν¬ μ—°κ²°μ΄ μ•μ •μ μΈμ§€ ν™•μΈ
4. **λ¨λΈ νΈν™μ„±**: μ΄μ „ λ¨λΈκ³Ό μƒλ΅μ΄ μ„¤μ •μ΄ νΈν™λλ”μ§€ ν™•μΈ

## π› λ¬Έμ  ν•΄κ²°

### λ¨λΈ λ΅λ“ μ‹¤ν¨
- MLflow μ„λ²„ μ—°κ²° ν™•μΈ
- λ¨λΈ μ΄λ¦„μ΄λ‚ run_idκ°€ μ¬λ°”λ¥Έμ§€ ν™•μΈ
- `--list-models` λλ” `--list-runs`λ΅ μ‚¬μ© κ°€λ¥ν• μµμ… ν™•μΈ

### ν•™μµ μ¤‘ μ¤λ¥
- GPU λ©”λ¨λ¦¬ λ¶€μ΅± μ‹ batch_size μ¤„μ΄κΈ°
- λ°μ΄ν„°μ…‹ κ²½λ΅ ν™•μΈ
- μμ΅΄μ„± ν¨ν‚¤μ§€ λ²„μ „ ν™•μΈ

## π“ μ„±λ¥ λ¨λ‹ν„°λ§

MLflowμ—μ„ λ‹¤μ λ©”νΈλ¦­μ„ μ¶”μ ν•  μ μμµλ‹λ‹¤:
- `step_loss`: κ° μ¤ν…λ³„ μ†μ‹¤
- `epoch_avg_loss`: μ—ν¬ν¬ ν‰κ·  μ†μ‹¤
- `final_loss`: μµμΆ… μ†μ‹¤
- `trainable_parameters`: ν•™μµ κ°€λ¥ν• νλΌλ―Έν„° μ
- `total_parameters`: μ „μ²΄ νλΌλ―Έν„° μ