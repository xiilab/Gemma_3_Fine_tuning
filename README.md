# Gemma-2b μ½”λ“ νμΈνλ‹ with MLflow μ—°μ† ν•™μµ

μ΄ ν”„λ΅μ νΈλ” Google Gemma-2b λ¨λΈμ„ μ½”λ“ λ°μ΄ν„°λ΅ νμΈνλ‹ν•κ³ , MLflowλ¥Ό ν†µν•΄ λ¨λΈμ„ κ΄€λ¦¬ν•λ©° μ—°μ† ν•™μµμ„ μ§€μ›ν•©λ‹λ‹¤.

## π€ μ£Όμ” κΈ°λ¥

- **QLoRA νμΈνλ‹**: λ©”λ¨λ¦¬ ν¨μ¨μ μΈ LoRA κΈ°λ° νμΈνλ‹
- **MLflow ν†µν•©**: μ‹¤ν— μ¶”μ , λ¨λΈ λ²„μ „ κ΄€λ¦¬, μ•„ν‹°ν©νΈ μ €μ¥
- **μ—°μ† ν•™μµ**: μ΄μ „ ν•™μµλ λ¨λΈμ„ λ΅λ“ν•μ—¬ μ¶”κ°€ ν•™μµ κ°€λ¥
- **λ°μ΄ν„°μ…‹ λ²”μ„ μ„ νƒ**: `dataset_start`μ™€ `dataset_end`λ΅ νΉμ • λ²”μ„μ λ°μ΄ν„°λ§ ν•™μµ
- **μλ™ λ¨λΈ μ €μ¥**: ν•™μµ μ™„λ£ ν›„ μλ™μΌλ΅ MLflowμ— λ¨λΈ λ“±λ΅

## π“‹ μ‚¬μ© λ°©λ²•

### 1. μƒλ΅μ΄ λ¨λΈλ΅ ν•™μµ μ‹μ‘

```bash
python main.py
```

### 2. MLflowμ—μ„ κΈ°μ΅΄ λ¨λΈμ„ λ΅λ“ν•μ—¬ μ—°μ† ν•™μµ

#### μ‚¬μ© κ°€λ¥ν• λ¨λΈ λ©λ΅ ν™•μΈ
```bash
python continue_training.py --list-models
```

#### μµκ·Ό ν•™μµ μ‹¤ν–‰ λ©λ΅ ν™•μΈ
```bash
python continue_training.py --list-runs
```

#### νΉμ • λ¨λΈ μ΄λ¦„μΌλ΅ μ—°μ† ν•™μµ
```bash
python continue_training.py --model-name gemma-2b-code-finetuned --epochs 2 --learning-rate 1e-4
```

#### νΉμ • run_idλ΅ μ—°μ† ν•™μµ
```bash
python continue_training.py --run-id abc123def456 --epochs 1 --batch-size 4
```

#### νΉμ • λ°μ΄ν„°μ…‹ λ²”μ„λ΅ μ—°μ† ν•™μµ
```bash
# 5000~15000 λ²”μ„μ λ°μ΄ν„°λ΅ μ—°μ† ν•™μµ
python continue_training.py --model-name gemma-2b-code-finetuned --dataset-start 5000 --dataset-end 15000

# 10000~20000 λ²”μ„μ λ°μ΄ν„°λ΅ μ—°μ† ν•™μµ  
python continue_training.py --model-name gemma-2b-code-finetuned --dataset-start 10000 --dataset-end 20000 --epochs 2
```

### 3. ν•μ΄νΌνλΌλ―Έν„° μ§μ ‘ μμ •

`main.py` νμΌμ—μ„ `hyperparams` λ”•μ…”λ„λ¦¬λ¥Ό μμ •ν•μ—¬ μ—°μ† ν•™μµ μ„¤μ •:

```python
hyperparams = {
    # ... κΈ°μ΅΄ μ„¤μ • ...
    "continue_from_model": "gemma-2b-code-finetuned",  # MLflowμ—μ„ λ΅λ“ν•  λ¨λΈ μ΄λ¦„
    "continue_from_run_id": None,  # λλ” νΉμ • run_id
    "num_epochs": 2,  # μ¶”κ°€ ν•™μµν•  μ—ν¬ν¬ μ
    "learning_rate": 1e-4,  # μƒλ΅μ΄ ν•™μµλ¥ 
    "dataset_start": 5000,  # λ°μ΄ν„°μ…‹ μ‹μ‘ μΈλ±μ¤
    "dataset_end": 15000,   # λ°μ΄ν„°μ…‹ λ μΈλ±μ¤ (exclusive)
}
```

## π”§ μ„¤μ •

### MLflow μ„λ²„ μ„¤μ •
- **μ„λ²„ μ£Όμ†**: http://10.61.3.161:30744/
- **μ‹¤ν— μ΄λ¦„**: Gemma-2b-Code-Finetuning
- **λ¨λΈ λ μ§€μ¤νΈλ¦¬**: μλ™μΌλ΅ λ¨λΈμ΄ λ“±λ΅λ¨

### Accelerate μ‚¬μ©λ²•
```bash
# 1λ‹¨κ³„: νλΌλ―Έν„° μ„¤μ •
python continue_training.py --model-name gemma-2b-code-finetuned --epochs 10 --learning-rate 1e-4 --dataset-start 0 --dataset-end 100000

# 2λ‹¨κ³„: accelerateλ΅ main.py μ‹¤ν–‰
accelerate launch main.py
```

### κΈ°λ³Έ ν•μ΄νΌνλΌλ―Έν„°
```python
{
    "model_name": "google/gemma-2b",
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "batch_size": 2,
    "learning_rate": 2e-4,
    "weight_decay": 0.01,
    "num_epochs": 1,
    "max_length": 512,
    "dataset_start": 0,  # λ°μ΄ν„°μ…‹ μ‹μ‘ μΈλ±μ¤
    "dataset_end": 10000,  # λ°μ΄ν„°μ…‹ λ μΈλ±μ¤ (exclusive)
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
}
```

## π“ MLflow UIμ—μ„ ν™•μΈ

ν•™μµ μ™„λ£ ν›„ MLflow UIμ—μ„ λ‹¤μμ„ ν™•μΈν•  μ μμµλ‹λ‹¤:

1. **μ‹¤ν— μ¶”μ **: http://10.61.3.161:30744/
2. **λ¨λΈ λ μ§€μ¤νΈλ¦¬**: λ“±λ΅λ λ¨λΈ λ²„μ „ κ΄€λ¦¬
3. **λ©”νΈλ¦­**: Loss, νλΌλ―Έν„° μ, ν•™μµ μ§„ν–‰ μƒν™©
4. **μ•„ν‹°ν©νΈ**: μ €μ¥λ λ¨λΈ νμΌ, μ„¤μ • νμΌ

## π”„ μ—°μ† ν•™μµ μ›ν¬ν”λ΅μ°

1. **μ΄κΈ° ν•™μµ**: `python main.py`λ΅ μ²« λ²μ§Έ ν•™μµ μ‹¤ν–‰
2. **λ¨λΈ ν™•μΈ**: MLflow UIμ—μ„ ν•™μµλ λ¨λΈ ν™•μΈ
3. **μ—°μ† ν•™μµ**: `continue_training.py`λ΅ μ¶”κ°€ ν•™μµ μ„¤μ •
   - λ¨λΈ μ„ νƒ: `--model-name` λλ” `--run-id`
   - λ°μ΄ν„°μ…‹ λ²”μ„: `--dataset-start`, `--dataset-end`
   - ν•™μµ νλΌλ―Έν„°: `--epochs`, `--learning-rate`, `--batch-size`
4. **μ¬μ‹¤ν–‰**: `accelerate launch main.py`λ΅ μ—°μ† ν•™μµ μ‹¤ν–‰
5. **λ°λ³µ**: ν•„μ”μ— λ”°λΌ λ‹¨κ³„ 2-4 λ°λ³µ

## π― Ollama λ³€ν™ μ›ν¬ν”λ΅μ°

1. **ν•™μµ μ™„λ£**: λ¨λΈ ν•™μµμ΄ μ™„λ£λλ©΄ λ΅μ»¬μ— λ³‘ν•©λ λ¨λΈ μƒμ„±
2. **μ—…λ΅λ“ ν™•μΈ**: MLflow μ—…λ΅λ“ μ„±κ³µ μ—¬λ¶€ ν™•μΈ
3. **λ³€ν™ λ°©λ²• μ„ νƒ**:
   - **MLflowμ—μ„ λ³€ν™**: `python mlflow_to_ollama_converter.py --model-name <MODEL_NAME>`
   - **λ΅μ»¬μ—μ„ μ§μ ‘ λ³€ν™**: `python mlflow_to_ollama_converter.py --local-model-path <PATH> --model-name <MODEL_NAME>`
4. **Ollama μ‹¤ν–‰**: `ollama run <MODEL_NAME>`

## π“ νμΌ κµ¬μ΅°

```
.
β”β”€β”€ main.py                        # λ©”μΈ ν•™μµ μ¤ν¬λ¦½νΈ
β”β”€β”€ continue_training.py           # μ—°μ† ν•™μµ μ„¤μ • λ„κµ¬
β”β”€β”€ retry_mlflow_upload.py         # MLflow μ—…λ΅λ“ μ¬μ‹λ„ μ¤ν¬λ¦½νΈ
β”β”€β”€ mlflow_to_ollama_converter.py  # MLflow λ¨λΈμ„ Ollamaλ΅ λ³€ν™
β”β”€β”€ README.md                      # μ΄ νμΌ
β”β”€β”€ CONVERTER_README.md            # λ³€ν™κΈ° μ‚¬μ©λ²•
β””β”€β”€ requirements.txt               # μμ΅΄μ„± ν¨ν‚¤μ§€
```

## β οΈ μ£Όμμ‚¬ν•­

1. **λ©”λ¨λ¦¬ μ”κµ¬μ‚¬ν•­**: GPU λ©”λ¨λ¦¬κ°€ μ¶©λ¶„ν•μ§€ ν™•μΈ
2. **λ°μ΄ν„°μ…‹ κ²½λ΅**: `/datasets/github-code/` κ²½λ΅μ— λ°μ΄ν„°μ…‹μ΄ μμ–΄μ•Ό ν•¨
3. **MLflow μ—°κ²°**: λ„¤νΈμ›ν¬ μ—°κ²°μ΄ μ•μ •μ μΈμ§€ ν™•μΈ
4. **λ¨λΈ νΈν™μ„±**: μ΄μ „ λ¨λΈκ³Ό μƒλ΅μ΄ μ„¤μ •μ΄ νΈν™λλ”μ§€ ν™•μΈ

## π› λ¬Έμ  ν•΄κ²°

### λ¨λΈ λ΅λ“ μ‹¤ν¨
- MLflow μ„λ²„ μ—°κ²° ν™•μΈ
- λ¨λΈ μ΄λ¦„μ΄λ‚ run_idκ°€ μ¬λ°”λ¥Έμ§€ ν™•μΈ
- `--list-models` λλ” `--list-runs`λ΅ μ‚¬μ© κ°€λ¥ν• μµμ… ν™•μΈ

### ν•™μµ μ¤‘ μ¤λ¥
- GPU λ©”λ¨λ¦¬ λ¶€μ΅± μ‹ batch_size μ¤„μ΄κΈ°
- λ°μ΄ν„°μ…‹ κ²½λ΅ ν™•μΈ
- μμ΅΄μ„± ν¨ν‚¤μ§€ λ²„μ „ ν™•μΈ

### MLflow μ—…λ΅λ“ Timeout μ¤λ¥ ν•΄κ²°

#### λ¬Έμ  μƒν™©
- ν° λ¨λΈ νμΌ (>1GB) μ—…λ΅λ“ μ‹ HTTP 500 μ¤λ¥ λλ” timeout λ°μƒ
- `ResponseError('too many 500 error responses')` μ—λ¬

#### ν•΄κ²° λ°©λ²•

**λ°©λ²• 1: μ¬μ‹λ„ μ¤ν¬λ¦½νΈ μ‚¬μ©**
```bash
# λ³„λ„ μ—…λ΅λ“ μ¤ν¬λ¦½νΈ μ‹¤ν–‰
python retry_mlflow_upload.py
```

**λ°©λ²• 2: μ§μ ‘ Ollama λ³€ν™ (MLflow μ°ν)**
```bash
# λ΅μ»¬ λ¨λΈμ„ μ§μ ‘ Ollamaλ΅ λ³€ν™
python mlflow_to_ollama_converter.py --local-model-path /datasets/github-code/gemma-2b-code-finetuned_merged --model-name gemma-2b-code-finetuned
```

**λ°©λ²• 3: κ°μ„ λ main.py μ‚¬μ©**
- μƒλ΅μ΄ `main.py`λ” μλ™μΌλ΅ νμΌ ν¬κΈ°λ¥Ό ν™•μΈν•κ³  μ μ ν• μ—…λ΅λ“ μ „λµ μ‚¬μ©
- 1GB μ΄μƒ λ¨λΈμ€ μλ™μΌλ΅ λ³„λ„ μ¤ν¬λ¦½νΈ μ‚¬μ© κ¶μ¥
- μ‘μ€ νμΌ(config, tokenizer)λ¶€ν„° λ‹¨κ³„μ  μ—…λ΅λ“

#### μ—…λ΅λ“ κ°μ„ μ‚¬ν•­
- **μ„λ²„ μƒνƒ ν™•μΈ**: μ—…λ΅λ“ μ „ MLflow μ„λ²„ health check
- **νμΌ ν¬κΈ° κΈ°λ° μ „λµ**: μ‘μ€ νμΌ μ°μ„  μ—…λ΅λ“
- **μ¬μ‹λ„ λ΅μ§**: μ‹¤ν¨ μ‹ μλ™ μ¬μ‹λ„ (μµλ€ 3ν)
- **μ²­ν¬ μ—…λ΅λ“**: ν° νμΌμ„ κ°λ³„μ μΌλ΅ μ²λ¦¬
- **νƒ€μ„μ•„μ›ƒ μ„¤μ •**: `MLFLOW_HTTP_REQUEST_TIMEOUT=300` (5λ¶„)

## π“ μ„±λ¥ λ¨λ‹ν„°λ§

MLflowμ—μ„ λ‹¤μ λ©”νΈλ¦­μ„ μ¶”μ ν•  μ μμµλ‹λ‹¤:
- `step_loss`: κ° μ¤ν…λ³„ μ†μ‹¤
- `epoch_avg_loss`: μ—ν¬ν¬ ν‰κ·  μ†μ‹¤
- `final_loss`: μµμΆ… μ†μ‹¤
- `trainable_parameters`: ν•™μµ κ°€λ¥ν• νλΌλ―Έν„° μ
- `total_parameters`: μ „μ²΄ νλΌλ―Έν„° μ