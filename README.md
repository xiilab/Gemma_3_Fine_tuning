# Gemma-3 Fine-tuning Project

Google Gemma-3 λ¨λΈμ„ ν•κµ­μ–΄ μ§μμ‘λ‹µ λ°μ΄ν„°μ…‹μΌλ΅ νμΈνλ‹ν•λ” ν”„λ΅μ νΈμ…λ‹λ‹¤. QLoRA(Quantized Low-Rank Adaptation) κΈ°λ²•μ„ μ‚¬μ©ν•μ—¬ ν¨μ¨μ μΈ νμΈνλ‹μ„ μν–‰ν•©λ‹λ‹¤.

## π“‹ ν”„λ΅μ νΈ κ°μ”

μ΄ ν”„λ΅μ νΈλ” λ‹¤μκ³Ό κ°™μ€ λ©μ μΌλ΅ κ°λ°λμ—μµλ‹λ‹¤:
- Google Gemma-3-4b-it λ¨λΈμ„ ν•κµ­μ–΄ μ§μμ‘λ‹µμ— νΉν™”ν•μ—¬ νμΈνλ‹
- KorQuAD/squad_kor_v1 λ°μ΄ν„°μ…‹μ„ ν™μ©ν• ν•κµ­μ–΄ μ΄ν•΄ λ¥λ ¥ ν–¥μƒ
- QLoRA κΈ°λ²•μ„ ν†µν• λ©”λ¨λ¦¬ ν¨μ¨μ μΈ νμΈνλ‹
- V100 GPU ν™κ²½μ—μ„μ μµμ ν™”λ ν•™μµ

## π€ μ£Όμ” νΉμ§•

- **λ¨λΈ**: Google Gemma-3-4b-it (Instruction-tuned)
- **λ°μ΄ν„°μ…‹**: KorQuAD/squad_kor_v1 (ν•κµ­μ–΄ μ§μμ‘λ‹µ)
- **νμΈνλ‹ κΈ°λ²•**: QLoRA (Quantized Low-Rank Adaptation)
- **GPU μ§€μ›**: NVIDIA V100, CUDA 12.2
- **ν”„λ μ„μ›ν¬**: Transformers, PEFT, Accelerate, MLflow

## π“ ν”„λ΅μ νΈ κµ¬μ΅°

```
Gemma_3_Fine_tuning/
β”β”€β”€ README.md                    # ν”„λ΅μ νΈ λ¬Έμ„
β”β”€β”€ main.py                      # νμΈνλ‹ μ‹¤ν–‰ μ¤ν¬λ¦½νΈ (MLflow μ—°λ™)
β”β”€β”€ load_model.py                # νμΈνλ‹λ λ¨λΈ ν…μ¤νΈ μ¤ν¬λ¦½νΈ
β”β”€β”€ mlflow_utils.py              # MLflow μ‹¤ν— κ΄€λ¦¬ μ ν‹Έλ¦¬ν‹°
β”β”€β”€ main.ipynb                   # μ‹¤ν—μ© λ…ΈνΈλ¶
β”β”€β”€ Gemma_3_Fine_tuning.ipynb   # λ©”μΈ νμΈνλ‹ λ…ΈνΈλ¶
β”β”€β”€ Dockerfile                   # Docker ν™κ²½ μ„¤μ •
β””β”€β”€ .ipynb_checkpoints/         # Jupyter μ²΄ν¬ν¬μΈνΈ
```

## π› οΈ μ„¤μΉ λ° ν™κ²½ μ„¤μ •

### 1. Dockerλ¥Ό μ‚¬μ©ν• ν™κ²½ κµ¬μ„± (κ¶μ¥)

```bash
# Docker μ΄λ―Έμ§€ λΉλ“
docker build -t gemma-finetuning .

# μ»¨ν…μ΄λ„ μ‹¤ν–‰ (GPU μ§€μ›)
docker run --gpus all -it -p 22:22 gemma-finetuning
```

### 2. μ§μ ‘ μ„¤μΉ

```bash
# Python 3.12 ν™κ²½ κ¶μ¥
pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu121

# ν•„μ ν¨ν‚¤μ§€ μ„¤μΉ
pip install transformers peft datasets accelerate tqdm bitsandbytes mlflow
```

## π― μ‚¬μ©λ²•

### 1. Python μ¤ν¬λ¦½νΈ μ‹¤ν–‰

```bash
accelerate launch main.py
```

### 2. MLflow UI μ‹¤ν–‰ (μ‹¤ν— μ¶”μ )

```bash
# MLflow UI μ ‘μ†
# λΈλΌμ°μ €μ—μ„ http://10.61.3.161:30744/ μ ‘μ†ν•μ—¬ μ‹¤ν— κ²°κ³Ό ν™•μΈ
```

### 3. Jupyter λ…ΈνΈλ¶ μ‚¬μ©

```bash
jupyter notebook Gemma_3_Fine_tuning.ipynb
```

### 4. νμΈνλ‹λ λ¨λΈ ν…μ¤νΈ

```bash
# ν•™μµ μ™„λ£ ν›„ λ¨λΈ ν…μ¤νΈ (MLflow λλ” λ΅μ»¬ νμΌμ—μ„ μλ™ λ΅λ“)
python load_model.py
```

### 5. MLflow μ‹¤ν— κ΄€λ¦¬

```bash
# MLflow μ‹¤ν— μ •λ³΄ μ΅°ν λ° κ΄€λ¦¬
python mlflow_utils.py

# μ‚¬μ© κ°€λ¥ν• κΈ°λ¥:
# - μ‹¤ν— λ©λ΅ μ΅°ν
# - μ‹¤ν–‰ λ©λ΅ μ΅°ν
# - μ‹¤ν–‰ μƒμ„Έ μ •λ³΄ μ΅°ν
# - λ“±λ΅λ λ¨λΈ λ©λ΅ μ΅°ν
# - μ•„ν‹°ν©νΈ λ‹¤μ΄λ΅λ“
```

### 6. Google Colabμ—μ„ μ‹¤ν–‰

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/100milliongold/Gemma_3_Fine_tuning/blob/main/Gemma_3_Fine_tuning.ipynb)

## β™οΈ μ£Όμ” μ„¤μ •

### QLoRA μ„¤μ •
```python
peft_config = LoraConfig(
    r=8,                    # Low-rank dimension
    lora_alpha=16,          # LoRA scaling parameter
    lora_dropout=0.1,       # Dropout probability
    bias="none",
    task_type=TaskType.CAUSAL_LM,
    target_modules=["q_proj", "k_proj", "v_proj", "o_proj", 
                   "gate_proj", "up_proj", "down_proj"]
)
```

### ν•™μµ νλΌλ―Έν„°
- **λ°°μΉ ν¬κΈ°**: 2
- **ν•™μµλ¥ **: 2e-4
- **μ—ν¬ν¬**: 1
- **μµλ€ μ‹ν€€μ¤ κΈΈμ΄**: 512
- **μµν‹°λ§μ΄μ €**: AdamW (weight_decay=0.01)

## π“ λ°μ΄ν„°μ…‹

**KorQuAD/squad_kor_v1** λ°μ΄ν„°μ…‹μ„ μ‚¬μ©ν•©λ‹λ‹¤:
- ν•κµ­μ–΄ μ§μμ‘λ‹µ λ°μ΄ν„°μ…‹
- SQuAD ν•μ‹μ ν•κµ­μ–΄ λ²„μ „
- μ§λ¬Έ-λ‹µλ³€ μμΌλ΅ κµ¬μ„±

### ν”„λ΅¬ν”„νΈ ν…ν”λ¦Ώ
```
Below is an instruction that describes a task, paired with an input that provides further context.
Write a response that appropriately completes the request.
Before answering, think carefully about the question and create a step-by-step chain of thoughts to ensure a logical and accurate response.

### Question:
{question}

### Response:
{answer}
```

## π”§ μ‹μ¤ν… μ”κµ¬μ‚¬ν•­

### μµμ† μ”κµ¬μ‚¬ν•­
- **GPU**: NVIDIA V100 (16GB VRAM) λλ” λ™κΈ‰
- **CUDA**: 12.1 μ΄μƒ
- **Python**: 3.12
- **RAM**: 32GB μ΄μƒ κ¶μ¥

### κ¶μ¥ ν™κ²½
- **OS**: Ubuntu 22.04
- **Docker**: μµμ‹  λ²„μ „
- **GPU λ“λΌμ΄λ²„**: μµμ‹  NVIDIA λ“λΌμ΄λ²„

## π“ μ„±λ¥ λ° κ²°κ³Ό

- **ν•™μµ κ°€λ¥ν• νλΌλ―Έν„°**: μ•½ 8Mκ° (μ „μ²΄ λ¨λΈμ μΌλ¶€λ§ ν•™μµ)
- **λ©”λ¨λ¦¬ μ‚¬μ©λ‰**: ~14GB VRAM (V100 κΈ°μ¤€)
- **ν•™μµ μ‹κ°„**: μ•½ 2-3μ‹κ°„ (10,000 μƒν” κΈ°μ¤€)

## π¤ κΈ°μ—¬ν•κΈ°

1. Fork the Project
2. Create your Feature Branch (`git checkout -b feature/AmazingFeature`)
3. Commit your Changes (`git commit -m 'Add some AmazingFeature'`)
4. Push to the Branch (`git push origin feature/AmazingFeature`)
5. Open a Pull Request

## π“ λΌμ΄μ„ μ¤

μ΄ ν”„λ΅μ νΈλ” MIT λΌμ΄μ„ μ¤ ν•μ— λ°°ν¬λ©λ‹λ‹¤.

## π™ κ°μ‚¬μ λ§

- **μ›λ³Έ μ¶μ²**: [Kaggle - Fine-tuning Gemma 3](https://www.kaggle.com/code/kingabzpro/fine-tuning-gemma-3-finq-a-reasoning)
- **μμ •**: webnautes (KorQuAD λ°μ΄ν„°μ…‹ μ μ©)
- **Hugging Face**: λ¨λΈ λ° λ°μ΄ν„°μ…‹ μ κ³µ
- **Google**: Gemma λ¨λΈ κ°λ°

## π“ λ¬Έμμ‚¬ν•­

ν”„λ΅μ νΈμ— λ€ν• μ§λ¬Έμ΄λ‚ μ μ•μ‚¬ν•­μ΄ μμΌμ‹λ©΄ Issueλ¥Ό μƒμ„±ν•΄ μ£Όμ„Έμ”.

---

**μ£Όμ**: μ΄ ν”„λ΅μ νΈλ” κµμ΅ λ° μ—°κµ¬ λ©μ μΌλ΅ μ μ‘λμ—μµλ‹λ‹¤. μƒμ—…μ  μ‚¬μ© μ‹μ—λ” κ΄€λ ¨ λΌμ΄μ„ μ¤λ¥Ό ν™•μΈν•΄ μ£Όμ„Έμ”.