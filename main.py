from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM,
    DataCollatorForLanguageModeling
)
from peft import LoraConfig, get_peft_model, TaskType
from datasets import load_dataset, Dataset
from torch.utils.data import DataLoader
from transformers import default_data_collator
from accelerate import Accelerator
from torch.optim import AdamW
from tqdm import tqdm
from itertools import islice
import torch
import mlflow
import mlflow.pytorch
import mlflow.transformers
import os
from datetime import datetime

# MLflow ì„¤ì •
mlflow.set_tracking_uri("http://10.61.3.161:30744/")  # ì›ê²© MLflow ì„œë²„ ì‚¬ìš©
experiment_name = "Gemma-2b-Code-Finetuning"
mlflow.set_experiment(experiment_name)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
hyperparams = {
    "model_name": "google/gemma-2b",
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "batch_size": 2,
    "learning_rate": 2e-4,
    "weight_decay": 0.01,
    "num_epochs": 1,
    "max_length": 512,
    "dataset_size": 10000,
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]
}

# MLflow ì‹¤í—˜ ì‹œì‘
with mlflow.start_run(run_name=f"gemma-finetuning-{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
    # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…
    mlflow.log_params(hyperparams)
    
    # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
    tokenizer = AutoTokenizer.from_pretrained(hyperparams["model_name"])
    model = AutoModelForCausalLM.from_pretrained(hyperparams["model_name"])

    # 2. QLoRA ì„¤ì •
    peft_config = LoraConfig(
        r=hyperparams["lora_r"],
        lora_alpha=hyperparams["lora_alpha"],
        lora_dropout=hyperparams["lora_dropout"],
        bias="none",
        task_type=TaskType.CAUSAL_LM,
        target_modules=hyperparams["target_modules"]
    )
    model = get_peft_model(model, peft_config)

    # 3. ë°ì´í„°ì…‹ ë¡œë”© ë° ì „ì²˜ë¦¬
    streamed = load_dataset(
        path="/datasets/github-code/github-code-clean",
        data_dir="/datasets/github-code/hf_data",
        cache_dir="/datasets/github-code/hf_cache",
        trust_remote_code=True,
        streaming=True
    )
    subset = list(islice(streamed["train"], hyperparams["dataset_size"]))
    dataset = Dataset.from_list(subset)

    # í…ìŠ¤íŠ¸ í¬ë§· ì •ì˜
    def format_example(example):
        code = example.get("code") or example.get("text") or example.get("content")
        language = example.get("language")
        return {"text": f"# {language.strip()} code snippet:\n{code}"}

    dataset = dataset.map(format_example)

    # í† í¬ë‚˜ì´ì§• ë° ë¼ë²¨ ì¶”ê°€
    def tokenize_and_add_labels(example):
        text = example.get("text")
        result = tokenizer(
            text,
            truncation=True,
            padding="max_length",
            max_length=hyperparams["max_length"],
            return_tensors="pt"
        )
        result = {k: v.squeeze(0) for k, v in result.items()}
        result["labels"] = result["input_ids"].clone()
        return result

    tokenized_dataset = dataset.map(
        tokenize_and_add_labels,
        batched=True,
        remove_columns=dataset.column_names
    )

    # 4. ë°ì´í„°ë¡œë” êµ¬ì„±
    train_dataloader = DataLoader(
        tokenized_dataset,
        batch_size=hyperparams["batch_size"],
        shuffle=True,
        collate_fn=default_data_collator
    )

    # 5. Optimizer ì„¤ì •
    optimizer = AdamW(
        filter(lambda p: p.requires_grad, model.parameters()),
        lr=hyperparams["learning_rate"],
        weight_decay=hyperparams["weight_decay"]
    )

    # 6. Accelerator ì´ˆê¸°í™”
    accelerator = Accelerator()
    model.config.use_cache = False
    model.enable_input_require_grads()  # gradient íë¦„ ë³´ì¥
    model.gradient_checkpointing_enable()
    model, train_dataloader, optimizer = accelerator.prepare(model, train_dataloader, optimizer)

    # í•™ìŠµ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸ ë° ë¡œê¹…
    trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
    total = sum(p.numel() for p in model.parameters())
    accelerator.print(f"Trainable parameters: {trainable} / {total}")
    
    # MLflowì— ëª¨ë¸ ì •ë³´ ë¡œê¹…
    mlflow.log_metric("trainable_parameters", trainable)
    mlflow.log_metric("total_parameters", total)
    mlflow.log_metric("trainable_ratio", trainable / total)

    # 7. í•™ìŠµ ë£¨í”„
    num_epochs = hyperparams["num_epochs"]
    global_step = 0
    
    for epoch in range(num_epochs):
        model.train()
        total_loss = 0
        epoch_losses = []
        
        for step, batch in enumerate(tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{num_epochs}")):
            outputs = model(**batch)
            loss = outputs.loss

            if not loss.requires_grad:
                raise RuntimeError("Loss does not require grad. Check labels and model mode.")

            accelerator.backward(loss)
            optimizer.step()
            optimizer.zero_grad()

            loss_value = loss.item()
            total_loss += loss_value
            epoch_losses.append(loss_value)
            global_step += 1

            # MLflowì— ìŠ¤í…ë³„ loss ë¡œê¹…
            if accelerator.is_main_process:
                mlflow.log_metric("step_loss", loss_value, step=global_step)

            if step % 50 == 0:
                accelerator.print(f"[Epoch {epoch+1}] Step {step} - Loss: {loss_value:.4f}")
                
                # ì¤‘ê°„ ë©”íŠ¸ë¦­ ë¡œê¹…
                if accelerator.is_main_process:
                    mlflow.log_metric("running_avg_loss", sum(epoch_losses) / len(epoch_losses), step=global_step)

        # ì—í¬í¬ ì™„ë£Œ ë©”íŠ¸ë¦­ ë¡œê¹…
        avg_loss = total_loss / len(train_dataloader)
        accelerator.print(f"===> Epoch {epoch+1} ì™„ë£Œ. í‰ê·  Loss: {avg_loss:.4f}")
        
        if accelerator.is_main_process:
            mlflow.log_metric("epoch_avg_loss", avg_loss, step=epoch+1)
            mlflow.log_metric("epoch_total_loss", total_loss, step=epoch+1)

    # 8. ëª¨ë¸ ì €ì¥ ë° MLflow ë“±ë¡
    if accelerator.is_main_process:
        # ë¡œì»¬ ì €ì¥
        output_dir = "/datasets/github-code/gemma-2b-code-finetuned"
        os.makedirs(output_dir, exist_ok=True)
        
        # ëª¨ë¸ unwrap ë° ì €ì¥
        unwrapped_model = accelerator.unwrap_model(model)
        unwrapped_model.save_pretrained(output_dir)
        tokenizer.save_pretrained(output_dir)
        
        print(f"ëª¨ë¸ì´ {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
        
        # MLflowì— ëª¨ë¸ ë“±ë¡
        try:
            # ìµœì¢… ë©”íŠ¸ë¦­ ë¡œê¹…
            final_loss = total_loss / len(train_dataloader)
            mlflow.log_metric("final_loss", final_loss)
            
            # ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ë¡œê¹…
            mlflow.log_artifacts(output_dir, "model")
            
            # í•™ìŠµ ì„¤ì • íŒŒì¼ ì €ì¥ ë° ë¡œê¹…
            config_file = "training_config.txt"
            with open(config_file, "w") as f:
                f.write("=== Gemma-2b Fine-tuning Configuration ===\n")
                for key, value in hyperparams.items():
                    f.write(f"{key}: {value}\n")
                f.write(f"\nFinal Results:\n")
                f.write(f"Final Loss: {final_loss:.4f}\n")
                f.write(f"Trainable Parameters: {trainable:,}\n")
                f.write(f"Total Parameters: {total:,}\n")
            
            mlflow.log_artifact(config_file)
            os.remove(config_file)  # ì„ì‹œ íŒŒì¼ ì‚­ì œ
            
            # ëª¨ë¸ì„ MLflow Model Registryì— ë“±ë¡
            model_name = "gemma-2b-code-finetuned"
            
            try:
                # 1. ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ì „ì²´ ë¡œê¹…
                mlflow.log_artifacts(output_dir, "model_files")
                
                # 2. PEFT ëª¨ë¸ì„ ìœ„í•œ ì»¤ìŠ¤í…€ ëª¨ë¸ í´ë˜ìŠ¤ ìƒì„±
                import tempfile
                import shutil
                
                # ì„ì‹œ ë””ë ‰í† ë¦¬ì— ëª¨ë¸ ì €ì¥ ì¤€ë¹„
                with tempfile.TemporaryDirectory() as temp_dir:
                    # ë² ì´ìŠ¤ ëª¨ë¸ ì •ë³´ ì €ì¥
                    base_model_info = {
                        "base_model": hyperparams["model_name"],
                        "peft_type": "LoRA",
                        "lora_config": {
                            "r": hyperparams["lora_r"],
                            "lora_alpha": hyperparams["lora_alpha"],
                            "lora_dropout": hyperparams["lora_dropout"],
                            "target_modules": hyperparams["target_modules"]
                        }
                    }
                    
                    # MLflow transformers í˜•ì‹ìœ¼ë¡œ ëª¨ë¸ ì €ì¥ ì‹œë„
                    try:
                        # ê°„ë‹¨í•œ ëª¨ë¸ ë˜í¼ ìƒì„±
                        model_wrapper = {
                            "model": unwrapped_model,
                            "tokenizer": tokenizer,
                            "base_model_name": hyperparams["model_name"],
                            "peft_config": base_model_info
                        }
                        
                        # transformers ëª¨ë¸ë¡œ ë“±ë¡
                        mlflow.transformers.log_model(
                            transformers_model={
                                "model": unwrapped_model,
                                "tokenizer": tokenizer
                            },
                            artifact_path="peft_model",
                            registered_model_name=model_name,
                            metadata=base_model_info,
                            pip_requirements=[
                                "torch>=2.0.0",
                                "transformers>=4.30.0",
                                "peft>=0.4.0",
                                "accelerate>=0.20.0"
                            ]
                        )
                        
                        print(f"âœ… ëª¨ë¸ì´ MLflow Model Registryì— '{model_name}' ì´ë¦„ìœ¼ë¡œ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤.")
                        
                    except Exception as transformers_error:
                        print(f"âš ï¸ Transformers í˜•ì‹ ë“±ë¡ ì‹¤íŒ¨: {transformers_error}")
                        
                        # ëŒ€ì•ˆ: íŒŒì¼ ê¸°ë°˜ ëª¨ë¸ ë“±ë¡
                        try:
                            # ëª¨ë¸ ì •ë³´ íŒŒì¼ ìƒì„±
                            model_info_file = "model_info.json"
                            import json
                            with open(model_info_file, "w") as f:
                                json.dump(base_model_info, f, indent=2)
                            
                            mlflow.log_artifact(model_info_file)
                            os.remove(model_info_file)
                            
                            # ëª¨ë¸ ê²½ë¡œ ì •ë³´ ì €ì¥
                            mlflow.log_param("model_save_path", output_dir)
                            mlflow.log_param("model_type", "peft_lora")
                            mlflow.log_param("model_registry_status", "artifacts_only")
                            
                            print(f"ğŸ“ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ê°€ MLflowì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤. ê²½ë¡œ: {output_dir}")
                            
                        except Exception as fallback_error:
                            print(f"âŒ ëª¨ë¸ ë“±ë¡ ì™„ì „ ì‹¤íŒ¨: {fallback_error}")
                            
            except Exception as main_error:
                print(f"âŒ MLflow ëª¨ë¸ ë“±ë¡ ì¤‘ ì£¼ìš” ì˜¤ë¥˜: {main_error}")
                # ìµœì†Œí•œì˜ ì •ë³´ë¼ë„ ì €ì¥
                mlflow.log_param("model_save_path", output_dir)
                mlflow.log_param("model_type", "peft_lora")
                mlflow.log_param("error_status", str(main_error))
            

            
        except Exception as e:
            print(f"MLflow ëª¨ë¸ ë“±ë¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            
    print("í•™ìŠµ ì™„ë£Œ! MLflow UIì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”: http://10.61.3.161:30744/")
