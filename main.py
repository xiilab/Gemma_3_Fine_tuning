from transformers import (
    AutoTokenizer,
    AutoModelForCausalLM
)
from transformers.data.data_collator import DataCollatorForLanguageModeling, default_data_collator
from peft import LoraConfig, get_peft_model, TaskType, PeftModel
from datasets import load_dataset, Dataset
from torch.utils.data import DataLoader
from accelerate import Accelerator
from torch.optim import AdamW
from tqdm import tqdm
from itertools import islice
import torch
import mlflow
import mlflow.pytorch
import os
from datetime import datetime
import json

# MLflow ì„¤ì •
import argparse
import sys

def get_mlflow_uri():
    """ëª…ë ¹í–‰ ì¸ìˆ˜ ë˜ëŠ” ê¸°ë³¸ê°’ìœ¼ë¡œ MLflow URI ê°€ì ¸ì˜¤ê¸°"""
    if '--mlflow-uri' in sys.argv:
        parser = argparse.ArgumentParser(add_help=False)
        parser.add_argument('--mlflow-uri', type=str, default="http://10.61.3.161:30366/")
        args, _ = parser.parse_known_args()
        return args.mlflow_uri
    return "http://10.61.3.161:30366/"

MLFLOW_URI = get_mlflow_uri()
mlflow.set_tracking_uri(MLFLOW_URI)
experiment_name = "Gemma-2b-Code-Finetuning"
mlflow.set_experiment(experiment_name)

# í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
hyperparams = {
    "model_name": "google/gemma-2b",
    "lora_r": 8,
    "lora_alpha": 16,
    "lora_dropout": 0.1,
    "batch_size": 2,
    "learning_rate": 2e-4,
    "weight_decay": 0.01,
    "num_epochs": 1,
    "max_length": 512,
    "dataset_start": 0,  # ë°ì´í„°ì…‹ ì‹œì‘ ì¸ë±ìŠ¤
    "dataset_end": 10000,  # ë°ì´í„°ì…‹ ë ì¸ë±ìŠ¤ (exclusive)
    "target_modules": ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"],
    "continue_from_model": None,  # MLflowì—ì„œ ê°€ì ¸ì˜¬ ëª¨ë¸ ì´ë¦„ (ì˜ˆ: "gemma-2b-code-finetuned")
    "continue_from_run_id": None,  # íŠ¹ì • run_idì—ì„œ ê°€ì ¸ì˜¬ ê²½ìš°
    "new_model_name": None  # ìƒˆë¡œìš´ ëª¨ë¸ëª… (Noneì´ë©´ ìë™ ìƒì„±)
}

def load_model_from_mlflow(model_name=None, run_id=None):
    """
    MLflowì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    try:
        if run_id:
            # íŠ¹ì • run_idì—ì„œ ëª¨ë¸ ë¡œë“œ
            print(f"MLflow run_id {run_id}ì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            logged_model = f"runs:/{run_id}/peft_model"
        elif model_name:
            # ìµœì‹  ë²„ì „ì˜ ëª¨ë¸ ë¡œë“œ
            print(f"MLflow Model Registryì—ì„œ '{model_name}' ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤...")
            logged_model = f"models:/{model_name}/latest"
        else:
            print("MLflowì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•˜ì§€ ì•Šê³  ìƒˆë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
            return None, None, None

        # MLflowì—ì„œ ëª¨ë¸ ë¡œë“œ
        loaded_model = mlflow.pytorch.load_model(logged_model)
        
        # ëª¨ë¸ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        from mlflow import MlflowClient
        client = MlflowClient()
        if run_id:
            run = client.get_run(run_id)
        else:
            # ìµœì‹  ëª¨ë¸ ë²„ì „ ì°¾ê¸°
            latest_version = client.get_latest_versions(model_name, stages=["None"])[0]
            run = client.get_run(latest_version.run_id)
        
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        previous_params = run.data.params
        print(f"ì´ì „ í•™ìŠµ í•˜ì´í¼íŒŒë¼ë¯¸í„°: {previous_params}")
        
        return loaded_model, previous_params, run_id or latest_version.run_id
        
    except Exception as e:
        print(f"MLflowì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        print("ìƒˆë¡œìš´ ëª¨ë¸ë¡œ ì‹œì‘í•©ë‹ˆë‹¤.")
        return None, None, None

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ ì •ì˜
def main():
    # MLflow ì‹¤í—˜ ì‹œì‘
    with mlflow.start_run(run_name=f"gemma-finetuning-{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
        # í•˜ì´í¼íŒŒë¼ë¯¸í„° ë¡œê¹…
        mlflow.log_params(hyperparams)
        
        # MLflowì—ì„œ ê¸°ì¡´ ëª¨ë¸ ë¡œë“œ ì‹œë„
        loaded_model, previous_params, source_run_id = load_model_from_mlflow(
            model_name=hyperparams.get("continue_from_model"),
            run_id=hyperparams.get("continue_from_run_id")
        )
        
        if loaded_model is not None:
            print("âœ… MLflowì—ì„œ ê¸°ì¡´ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤.")
            model = loaded_model
            tokenizer = AutoTokenizer.from_pretrained(hyperparams["model_name"])
            
            # ì´ì „ í•™ìŠµ ì •ë³´ ë¡œê¹…
            if source_run_id:
                mlflow.log_param("continued_from_run_id", source_run_id)
            if previous_params:
                mlflow.log_param("previous_training_params", json.dumps(previous_params))
        else:
            print("ğŸ†• ìƒˆë¡œìš´ ëª¨ë¸ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤.")
            # 1. ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë”©
            tokenizer = AutoTokenizer.from_pretrained(hyperparams["model_name"])
            model = AutoModelForCausalLM.from_pretrained(hyperparams["model_name"])

            # 2. QLoRA ì„¤ì •
            peft_config = LoraConfig(
                r=hyperparams["lora_r"],
                lora_alpha=hyperparams["lora_alpha"],
                lora_dropout=hyperparams["lora_dropout"],
                bias="none",
                task_type=TaskType.CAUSAL_LM,
                target_modules=hyperparams["target_modules"]
            )
            model = get_peft_model(model, peft_config)

        # 3. ë°ì´í„°ì…‹ ë¡œë”© ë° ì „ì²˜ë¦¬
        streamed = load_dataset(
            path="/datasets/github-code/github-code-clean",
            data_dir="/datasets/github-code/hf_data",
            cache_dir="/datasets/github-code/hf_cache",
            trust_remote_code=True,
            streaming=True
        )
        # ìŠ¤íŠ¸ë¦¬ë° ë°ì´í„°ì…‹ì—ì„œ ë°ì´í„° ì¶”ì¶œ (start ~ end ë²”ìœ„)
        subset = []
        start_idx = hyperparams["dataset_start"]
        end_idx = hyperparams["dataset_end"]
        
        # startë¶€í„° endê¹Œì§€ì˜ ë°ì´í„°ë§Œ ì¶”ì¶œ
        for i, item in enumerate(islice(streamed["train"], end_idx)):
            if i >= start_idx:
                subset.append(item)
        dataset = Dataset.from_list(subset)

        # í…ìŠ¤íŠ¸ í¬ë§· ì •ì˜
        def format_example(example):
            code = example.get("code") or example.get("text") or example.get("content")
            language = example.get("language", "unknown")
            return {"text": f"# {language.strip()} code snippet:\n{code}"}

        dataset = dataset.map(format_example)

        # í† í¬ë‚˜ì´ì§• ë° ë¼ë²¨ ì¶”ê°€
        def tokenize_and_add_labels(example):
            text = example.get("text", "")
            result = tokenizer(
                text,
                truncation=True,
                padding="max_length",
                max_length=hyperparams["max_length"],
                return_tensors="pt"
            )
            result = {k: v.squeeze(0) for k, v in result.items()}
            result["labels"] = result["input_ids"].clone()
            return result

        tokenized_dataset = dataset.map(
            tokenize_and_add_labels,
            batched=True,
            remove_columns=dataset.column_names
        )

        # 4. ë°ì´í„°ë¡œë” êµ¬ì„±
        train_dataloader = DataLoader(
            tokenized_dataset,
            batch_size=hyperparams["batch_size"],
            shuffle=True,
            collate_fn=default_data_collator
        )

        # 5. Optimizer ì„¤ì •
        optimizer = AdamW(
            filter(lambda p: p.requires_grad, model.parameters()),
            lr=hyperparams["learning_rate"],
            weight_decay=hyperparams["weight_decay"]
        )

        # 6. Accelerator ì´ˆê¸°í™”
        accelerator = Accelerator()
        
        # ëª¨ë¸ ì„¤ì •
        if hasattr(model, 'config'):
            model.config.use_cache = False
        model.enable_input_require_grads()  # gradient íë¦„ ë³´ì¥
        model.gradient_checkpointing_enable()
        model, train_dataloader, optimizer = accelerator.prepare(model, train_dataloader, optimizer)

        # í•™ìŠµ íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸ ë° ë¡œê¹…
        trainable = sum(p.numel() for p in model.parameters() if p.requires_grad)
        total = sum(p.numel() for p in model.parameters())
        accelerator.print(f"Trainable parameters: {trainable} / {total}")
        
        # MLflowì— ëª¨ë¸ ì •ë³´ ë¡œê¹…
        mlflow.log_metric("trainable_parameters", trainable)
        mlflow.log_metric("total_parameters", total)
        mlflow.log_metric("trainable_ratio", trainable / total)

        # 7. í•™ìŠµ ë£¨í”„
        num_epochs = hyperparams["num_epochs"]
        global_step = 0
        
        for epoch in range(num_epochs):
            model.train()
            total_loss = 0
            epoch_losses = []
            
            for step, batch in enumerate(tqdm(train_dataloader, desc=f"Epoch {epoch+1}/{num_epochs}")):
                outputs = model(**batch)
                loss = outputs.loss

                if not loss.requires_grad:
                    raise RuntimeError("Loss does not require grad. Check labels and model mode.")

                accelerator.backward(loss)
                optimizer.step()
                optimizer.zero_grad()

                loss_value = loss.item()
                total_loss += loss_value
                epoch_losses.append(loss_value)
                global_step += 1

                # MLflowì— ìŠ¤í…ë³„ loss ë¡œê¹…
                if accelerator.is_main_process:
                    mlflow.log_metric("step_loss", loss_value, step=global_step)

                if step % 50 == 0:
                    accelerator.print(f"[Epoch {epoch+1}] Step {step} - Loss: {loss_value:.4f}")
                    
                    # ì¤‘ê°„ ë©”íŠ¸ë¦­ ë¡œê¹…
                    if accelerator.is_main_process:
                        mlflow.log_metric("running_avg_loss", sum(epoch_losses) / len(epoch_losses), step=global_step)

            # ì—í¬í¬ ì™„ë£Œ ë©”íŠ¸ë¦­ ë¡œê¹…
            avg_loss = total_loss / len(train_dataloader)
            accelerator.print(f"===> Epoch {epoch+1} ì™„ë£Œ. í‰ê·  Loss: {avg_loss:.4f}")
            
            if accelerator.is_main_process:
                mlflow.log_metric("epoch_avg_loss", avg_loss, step=epoch+1)
                mlflow.log_metric("epoch_total_loss", total_loss, step=epoch+1)

        # 8. ëª¨ë¸ ì €ì¥ ë° MLflow ë“±ë¡
        if accelerator.is_main_process:
            # ëª¨ë¸ëª… ê²°ì • ë¡œì§
            if hyperparams.get("new_model_name"):
                # ì‚¬ìš©ìê°€ ì§€ì •í•œ ìƒˆë¡œìš´ ëª¨ë¸ëª… ì‚¬ìš©
                model_name = hyperparams["new_model_name"]
            elif hyperparams.get("continue_from_model"):
                # ì—°ì† í•™ìŠµì¸ ê²½ìš°: ê¸°ì¡´ ëª¨ë¸ëª… ì‚¬ìš©
                model_name = hyperparams["continue_from_model"]
            else:
                # ìƒˆë¡œìš´ í•™ìŠµì¸ ê²½ìš°: ìë™ ìƒì„±
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                base_model = hyperparams["model_name"].split("/")[-1]  # "google/gemma-2b" -> "gemma-2b"
                model_name = f"{base_model}-finetuned-{timestamp}"
            
            output_dir = f"/datasets/github-code/{model_name}"
            os.makedirs(output_dir, exist_ok=True)
            
            print(f"ğŸ“ ëª¨ë¸ ì €ì¥ ê²½ë¡œ: {output_dir}")
            print(f"ğŸ·ï¸ ëª¨ë¸ëª…: {model_name}")
            
            # ëª¨ë¸ unwrap
            unwrapped_model = accelerator.unwrap_model(model)
            
            # Ollama í˜¸í™˜ì„ ìœ„í•œ ì™„ì „í•œ ëª¨ë¸ ìƒì„±
            print("ğŸ”„ Ollama í˜¸í™˜ ëª¨ë¸ ìƒì„± ì¤‘...")
            
            try:
                # LoRA ì–´ëŒ‘í„°ë¥¼ ê¸°ë³¸ ëª¨ë¸ì— ë³‘í•©
                merged_model = unwrapped_model.merge_and_unload()
                print("âœ… LoRA ì–´ëŒ‘í„°ê°€ ê¸°ë³¸ ëª¨ë¸ì— ì„±ê³µì ìœ¼ë¡œ ë³‘í•©ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                # ë³‘í•©ëœ ëª¨ë¸ ì €ì¥ (Ollama í˜¸í™˜)
                ollama_output_dir = f"{output_dir}_merged"
                os.makedirs(ollama_output_dir, exist_ok=True)
                
                merged_model.save_pretrained(ollama_output_dir)
                tokenizer.save_pretrained(ollama_output_dir)
                
                print(f"âœ… ë³‘í•©ëœ ëª¨ë¸ì´ {ollama_output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤ (Ollama í˜¸í™˜).")
                
                # ì›ë³¸ LoRA ëª¨ë¸ë„ ë³„ë„ ì €ì¥
                unwrapped_model.save_pretrained(output_dir)
                tokenizer.save_pretrained(output_dir)
                print(f"âœ… LoRA ëª¨ë¸ì´ {output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                
            except Exception as merge_error:
                print(f"âš ï¸ ëª¨ë¸ ë³‘í•© ì‹¤íŒ¨: {merge_error}")
                print("LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥í•©ë‹ˆë‹¤.")
                
                # ë³‘í•© ì‹¤íŒ¨ ì‹œ ì›ë³¸ ëª¨ë¸ë§Œ ì €ì¥
                unwrapped_model.save_pretrained(output_dir)
                tokenizer.save_pretrained(output_dir)
                ollama_output_dir = None
            
            # MLflowì— ëª¨ë¸ ë“±ë¡ (timeout ë°©ì§€ë¥¼ ìœ„í•œ ê°œì„ )
            try:
                # ìµœì¢… ë©”íŠ¸ë¦­ ë¡œê¹…
                final_loss = total_loss / len(train_dataloader)
                mlflow.log_metric("final_loss", final_loss)
                
                # ëª¨ë¸ ì—…ë¡œë“œ ì „ ì„œë²„ ìƒíƒœ í™•ì¸
                import requests
                try:
                    health_url = f"{MLFLOW_URI.rstrip('/')}/health"
                    response = requests.get(health_url, timeout=10)
                    if response.status_code != 200:
                        print("âš ï¸ MLflow ì„œë²„ ìƒíƒœê°€ ë¶ˆì•ˆì •í•©ë‹ˆë‹¤. ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
                        raise Exception("MLflow server health check failed")
                except requests.RequestException as e:
                    print(f"âš ï¸ MLflow ì„œë²„ ì—°ê²° í™•ì¸ ì‹¤íŒ¨: {e}")
                    raise Exception("MLflow server connection failed")
                
                # íŒŒì¼ í¬ê¸° í™•ì¸ ë° ì—…ë¡œë“œ ì „ëµ ê²°ì •
                def get_dir_size(path):
                    total_size = 0
                    for dirpath, dirnames, filenames in os.walk(path):
                        for filename in filenames:
                            filepath = os.path.join(dirpath, filename)
                            total_size += os.path.getsize(filepath)
                    return total_size
                
                # 1. ì‘ì€ íŒŒì¼ë“¤ ë¨¼ì € ì—…ë¡œë“œ (config, tokenizer ë“±)
                print("ğŸ“¤ ì„¤ì • íŒŒì¼ë“¤ì„ ë¨¼ì € ì—…ë¡œë“œ ì¤‘...")
                small_files = ["config.json", "tokenizer.json", "tokenizer_config.json", 
                              "special_tokens_map.json", "generation_config.json"]
                
                upload_dir = ollama_output_dir if ollama_output_dir and os.path.exists(ollama_output_dir) else output_dir
                
                for filename in small_files:
                    filepath = os.path.join(upload_dir, filename)
                    if os.path.exists(filepath):
                        try:
                            mlflow.log_artifact(filepath, "model")
                            print(f"   âœ… {filename} ì—…ë¡œë“œ ì™„ë£Œ")
                        except Exception as e:
                            print(f"   âŒ {filename} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                
                # 2. ëª¨ë¸ íƒ€ì… ë° ë©”íƒ€ë°ì´í„° ë¡œê¹…
                if ollama_output_dir and os.path.exists(ollama_output_dir):
                    mlflow.log_param("model_type", "merged_full_model")
                    mlflow.log_param("ollama_compatible", True)
                    model_size = get_dir_size(ollama_output_dir)
                else:
                    mlflow.log_param("model_type", "lora_adapter")
                    mlflow.log_param("ollama_compatible", False)
                    model_size = get_dir_size(output_dir)
                
                model_size_mb = model_size / (1024 * 1024)
                mlflow.log_metric("model_size_mb", model_size_mb)
                
                # 3. í° íŒŒì¼ ì—…ë¡œë“œ (safetensors ë“±) - ì¡°ê±´ë¶€ ì—…ë¡œë“œ
                if model_size_mb > 1000:  # 1GB ì´ìƒì¸ ê²½ìš°
                    print(f"âš ï¸ ëª¨ë¸ í¬ê¸°ê°€ {model_size_mb:.1f}MBë¡œ í½ë‹ˆë‹¤.")
                    print("   í° íŒŒì¼ ì—…ë¡œë“œëŠ” ë³„ë„ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”: python retry_mlflow_upload.py")
                    mlflow.log_param("large_model_upload_required", True)
                    mlflow.log_param("upload_script", "retry_mlflow_upload.py")
                else:
                    print("ğŸ“¤ ì „ì²´ ëª¨ë¸ ì—…ë¡œë“œ ì‹œë„ ì¤‘...")
                    try:
                        # íƒ€ì„ì•„ì›ƒ ì„¤ì •ì„ ìœ„í•œ í™˜ê²½ë³€ìˆ˜ ì„¤ì •
                        os.environ['MLFLOW_HTTP_REQUEST_TIMEOUT'] = '300'  # 5ë¶„
                        
                        if ollama_output_dir and os.path.exists(ollama_output_dir):
                            mlflow.log_artifacts(ollama_output_dir, "model")
                            print("âœ… ë³‘í•©ëœ ëª¨ë¸ì´ MLflowì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                        else:
                            mlflow.log_artifacts(output_dir, "model")
                            print("âœ… LoRA ëª¨ë¸ì´ MLflowì— ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                            
                    except Exception as upload_error:
                        print(f"âŒ í° íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {upload_error}")
                        print("   ë³„ë„ ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”: python retry_mlflow_upload.py")
                        mlflow.log_param("upload_failed", True)
                        mlflow.log_param("upload_error", str(upload_error))
                
                # 4. LoRA ì–´ëŒ‘í„° ë°±ì—… (ì‘ì€ íŒŒì¼ë“¤ë§Œ)
                if ollama_output_dir and os.path.exists(output_dir):
                    print("ğŸ“¤ LoRA ì–´ëŒ‘í„° ë°±ì—… ì¤‘...")
                    try:
                        # LoRA ì–´ëŒ‘í„°ì˜ ì‘ì€ íŒŒì¼ë“¤ë§Œ ì—…ë¡œë“œ
                        for filename in os.listdir(output_dir):
                            filepath = os.path.join(output_dir, filename)
                            if os.path.isfile(filepath):
                                file_size = os.path.getsize(filepath) / (1024 * 1024)
                                if file_size < 10:  # 10MB ë¯¸ë§Œ íŒŒì¼ë§Œ
                                    mlflow.log_artifact(filepath, "peft_adapter")
                    except Exception as e:
                        print(f"âš ï¸ LoRA ì–´ëŒ‘í„° ë°±ì—… ì‹¤íŒ¨: {e}")
                
                # 3. ëª¨ë¸ ì •ë³´ íŒŒì¼ ìƒì„± ë° ì €ì¥
                model_info = {
                    "base_model": hyperparams["model_name"],
                    "model_type": "merged_full_model" if ollama_output_dir else "lora_adapter",
                    "ollama_compatible": bool(ollama_output_dir),
                    "lora_config": {
                        "r": hyperparams["lora_r"],
                        "lora_alpha": hyperparams["lora_alpha"],
                        "lora_dropout": hyperparams["lora_dropout"],
                        "target_modules": hyperparams["target_modules"]
                    },
                    "training_config": hyperparams,
                    "final_loss": final_loss,
                    "trainable_parameters": trainable,
                    "total_parameters": total
                }
                
                model_info_file = "model_info.txt"
                with open(model_info_file, "w") as f:
                    f.write("=== Gemma-2b Fine-tuning Model Information ===\n\n")
                    f.write(f"Model Type: {model_info['model_type']}\n")
                    f.write(f"Ollama Compatible: {model_info['ollama_compatible']}\n")
                    f.write(f"Base Model: {model_info['base_model']}\n")
                    f.write(f"Final Loss: {model_info['final_loss']:.4f}\n")
                    f.write(f"Trainable Parameters: {model_info['trainable_parameters']:,}\n")
                    f.write(f"Total Parameters: {model_info['total_parameters']:,}\n\n")
                    
                    f.write("=== LoRA Configuration ===\n")
                    for key, value in model_info['lora_config'].items():
                        f.write(f"{key}: {value}\n")
                    
                    f.write("\n=== Training Configuration ===\n")
                    for key, value in model_info['training_config'].items():
                        f.write(f"{key}: {value}\n")
                    
                    if ollama_output_dir:
                        f.write(f"\n=== Ollama ì‚¬ìš©ë²• ===\n")
                        f.write(f"1. MLflowì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ\n")
                        f.write(f"2. python mlflow_to_ollama_converter.py --run-id <RUN_ID> --model-name <MODEL_NAME>\n")
                        f.write(f"3. ollama run <MODEL_NAME>\n")
                
                mlflow.log_artifact(model_info_file)
                os.remove(model_info_file)
                
                # 4. í•™ìŠµ ì„¤ì • íŒŒì¼ ì €ì¥
                config_file = "training_config.txt"
                with open(config_file, "w") as f:
                    f.write("=== Gemma-2b Fine-tuning Configuration ===\n")
                    for key, value in hyperparams.items():
                        f.write(f"{key}: {value}\n")
                    f.write(f"\nFinal Results:\n")
                    f.write(f"Final Loss: {final_loss:.4f}\n")
                    f.write(f"Trainable Parameters: {trainable:,}\n")
                    f.write(f"Total Parameters: {total:,}\n")
                    f.write(f"Model Type: {'Merged (Ollama Compatible)' if ollama_output_dir else 'LoRA Adapter Only'}\n")
                
                mlflow.log_artifact(config_file)
                os.remove(config_file)
                
                # 5. ì¶”ê°€ ë©”íƒ€ë°ì´í„° ë¡œê¹…
                mlflow.log_param("model_save_path", ollama_output_dir if ollama_output_dir else output_dir)
                mlflow.log_param("lora_save_path", output_dir)
                mlflow.log_param("base_model_name", hyperparams["model_name"])
                
                # LoRA ì„¤ì • ë¡œê¹…
                for key, value in model_info['lora_config'].items():
                    mlflow.log_param(f"lora_{key}", value)
                
                print("âœ… ëª¨ë“  ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ê°€ MLflowì— ì„±ê³µì ìœ¼ë¡œ ì—…ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤.")
                
                if ollama_output_dir:
                    print(f"ğŸ‰ Ollama í˜¸í™˜ ëª¨ë¸ì´ ì¤€ë¹„ë˜ì—ˆìŠµë‹ˆë‹¤!")
                    print(f"ë³€í™˜ ëª…ë ¹: python mlflow_to_ollama_converter.py --run-id <RUN_ID> --model-name <MODEL_NAME>")
                else:
                    print("âš ï¸ ëª¨ë¸ ë³‘í•©ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. LoRA ì–´ëŒ‘í„°ë§Œ ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
                    print("Ollama ë³€í™˜ì„ ìœ„í•´ì„œëŠ” ìˆ˜ë™ìœ¼ë¡œ ëª¨ë¸ì„ ë³‘í•©í•´ì•¼ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
                
            except Exception as e:
                print(f"âŒ MLflow ëª¨ë¸ ë“±ë¡ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
                # ìµœì†Œí•œì˜ ì •ë³´ë¼ë„ ì €ì¥
                mlflow.log_param("model_save_path", output_dir)
                mlflow.log_param("error_status", str(e))
                
        print(f"í•™ìŠµ ì™„ë£Œ! MLflow UIì—ì„œ ê²°ê³¼ë¥¼ í™•ì¸í•˜ì„¸ìš”: {MLFLOW_URI}")

# ì§ì ‘ ì‹¤í–‰ë  ë•Œë§Œ í•™ìŠµ ì‹œì‘
if __name__ == "__main__":
    main()
