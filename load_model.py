from transformers import AutoTokenizer, AutoModelForCausalLM
from peft import PeftModel
import torch
import mlflow
import mlflow.transformers
import os

def load_from_mlflow(model_name="gemma-2b-code-finetuned", version="latest"):
    """MLflow Model Registryì—ì„œ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        # MLflow ì„¤ì •
        mlflow.set_tracking_uri("http://10.61.3.161:30744/")
        
        # ë“±ë¡ëœ ëª¨ë¸ ë¡œë“œ ì‹œë„
        model_uri = f"models:/{model_name}/{version}"
        
        try:
            # transformers ëª¨ë¸ë¡œ ë¡œë“œ ì‹œë„
            loaded_model = mlflow.transformers.load_model(model_uri)
            print(f"âœ… MLflowì—ì„œ ëª¨ë¸ì„ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œí–ˆìŠµë‹ˆë‹¤: {model_name}")
            return loaded_model["model"], loaded_model["tokenizer"]
        except Exception as e:
            print(f"âš ï¸ MLflow transformers ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None, None
            
    except Exception as e:
        print(f"âŒ MLflow ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

def load_finetuned_model(model_path="/datasets/github-code/gemma-2b-code-finetuned"):
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ ë¡œë“œí•©ë‹ˆë‹¤."""
    try:
        # í† í¬ë‚˜ì´ì € ë¡œë“œ
        tokenizer = AutoTokenizer.from_pretrained(model_path)
        
        # ë² ì´ìŠ¤ ëª¨ë¸ ë¡œë“œ
        base_model = AutoModelForCausalLM.from_pretrained("google/gemma-2b")
        
        # PEFT ëª¨ë¸ ë¡œë“œ
        model = PeftModel.from_pretrained(base_model, model_path)
        
        print(f"âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤: {model_path}")
        return model, tokenizer
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return None, None

def test_model(model, tokenizer, prompt="# Python code snippet:\ndef hello_world():"):
    """ëª¨ë¸ì„ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤."""
    if model is None or tokenizer is None:
        print("âŒ ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        return
    
    try:
        # ì…ë ¥ í† í¬ë‚˜ì´ì§•
        inputs = tokenizer(prompt, return_tensors="pt")
        
        # ìƒì„±
        with torch.no_grad():
            outputs = model.generate(
                input_ids=inputs.input_ids,
                attention_mask=inputs.attention_mask,
                max_new_tokens=100,
                do_sample=True,
                temperature=0.7,
                pad_token_id=tokenizer.eos_token_id
            )
        
        # ê²°ê³¼ ë””ì½”ë”©
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        
        print("ğŸ” ì…ë ¥:")
        print(prompt)
        print("\nğŸ“ ìƒì„±ëœ ì½”ë“œ:")
        print(generated_text[len(prompt):])
        
    except Exception as e:
        print(f"âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")

if __name__ == "__main__":
    print("ğŸš€ íŒŒì¸íŠœë‹ëœ Gemma-2b ëª¨ë¸ í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    # 1. MLflowì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹œë„
    print("ğŸ“¡ MLflow Model Registryì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹œë„...")
    model, tokenizer = load_from_mlflow()
    
    # 2. MLflow ë¡œë“œ ì‹¤íŒ¨ ì‹œ ë¡œì»¬ íŒŒì¼ì—ì„œ ë¡œë“œ
    if model is None or tokenizer is None:
        print("ğŸ“ ë¡œì»¬ íŒŒì¼ì—ì„œ ëª¨ë¸ ë¡œë“œ ì‹œë„...")
        model, tokenizer = load_finetuned_model()
    
    if model and tokenizer:
        # í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ë“¤
        test_prompts = [
            "# Python code snippet:\ndef calculate_fibonacci(n):",
            "# Python code snippet:\nclass DataProcessor:",
            "# Python code snippet:\nimport pandas as pd\n\ndef process_data():"
        ]
        
        for i, prompt in enumerate(test_prompts, 1):
            print(f"\nğŸ“‹ í…ŒìŠ¤íŠ¸ {i}:")
            print("-" * 30)
            test_model(model, tokenizer, prompt)
            print() 