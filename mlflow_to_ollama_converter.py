#!/usr/bin/env python3
"""
MLflow ëª¨ë¸ì„ Ollama ëª¨ë¸ë¡œ ë³€í™˜í•˜ëŠ” ìŠ¤í¬ë¦½íŠ¸
ì£¼ì¸ë‹˜ì˜ íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì„ Ollamaì—ì„œ ì‚¬ìš©í•  ìˆ˜ ìˆë„ë¡ ë³€í™˜í•©ë‹ˆë‹¤.
"""

import os
import json
import shutil
import subprocess
import argparse
import time
import requests
from pathlib import Path
from datetime import datetime
import mlflow
from mlflow.tracking import MlflowClient
import tempfile

class MLflowToOllamaConverter:
    def __init__(self, mlflow_tracking_uri="http://10.61.3.161:30366/"):
        self.mlflow_tracking_uri = mlflow_tracking_uri
        self.client = None
        self.setup_mlflow()
    
    def setup_mlflow(self):
        """MLflow í´ë¼ì´ì–¸íŠ¸ ì„¤ì •"""
        try:
            mlflow.set_tracking_uri(self.mlflow_tracking_uri)
            self.client = MlflowClient()
            print(f"âœ… MLflow í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì™„ë£Œ: {self.mlflow_tracking_uri}")
        except Exception as e:
            print(f"âŒ MLflow í´ë¼ì´ì–¸íŠ¸ ì„¤ì • ì‹¤íŒ¨: {e}")
            self.client = None
    
    def test_mlflow_connection(self):
        """MLflow ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸"""
        try:
            print("ğŸ” MLflow ì„œë²„ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘...")
            
            if self.client is None:
                print("âŒ MLflow í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return False
            
            # ê¸°ë³¸ ì—°ê²° í…ŒìŠ¤íŠ¸
            experiments = self.client.search_experiments(max_results=1)
            print("âœ… MLflow ì„œë²„ ì—°ê²° ì„±ê³µ!")
            return True
            
        except Exception as e:
            print(f"âŒ MLflow ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
            
            # ì„œë²„ ìƒíƒœ í™•ì¸
            try:
                response = requests.get(f"{self.mlflow_tracking_uri}/health", timeout=10)
                if response.status_code == 200:
                    print("ğŸ”§ MLflow ì„œë²„ëŠ” ì‹¤í–‰ ì¤‘ì´ì§€ë§Œ API ì ‘ê·¼ì— ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤.")
                else:
                    print(f"ğŸ”§ MLflow ì„œë²„ ìƒíƒœ: {response.status_code}")
            except:
                print("ğŸ”§ MLflow ì„œë²„ì— ì ‘ê·¼í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            return False
    
    def list_available_models(self, experiment_name="Gemma-2b-Code-Finetuning"):
        """ë³€í™˜ ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            if not self.test_mlflow_connection():
                return []
                
            if self.client is None:
                return []
                
            experiment = self.client.get_experiment_by_name(experiment_name)
            if experiment is None:
                print(f"âŒ ì‹¤í—˜ '{experiment_name}'ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return []
            
            runs = self.client.search_runs(
                experiment_ids=[experiment.experiment_id],
                filter_string="attribute.status = 'FINISHED'",
                order_by=["start_time DESC"],
                max_results=20
            )
            
            print(f"ğŸ” '{experiment_name}' ì‹¤í—˜ì˜ ì™„ë£Œëœ ëª¨ë¸ ëª©ë¡:")
            print("=" * 80)
            
            available_models = []
            for i, run in enumerate(runs):
                try:
                    # ì•„í‹°íŒ©íŠ¸ ì¡´ì¬ ì—¬ë¶€ í™•ì¸
                    artifacts = self.client.list_artifacts(run.info.run_id)
                    if not artifacts:
                        print(f"âš ï¸ Run {run.info.run_id[:8]}... - ì•„í‹°íŒ©íŠ¸ ì—†ìŒ (ê±´ë„ˆëœ€)")
                        continue
                    
                    run_info = {
                        'index': i + 1,
                        'run_id': run.info.run_id,
                        'run_name': run.data.tags.get('mlflow.runName', 'N/A'),
                        'start_time': datetime.fromtimestamp(run.info.start_time/1000),
                        'metrics': run.data.metrics,
                        'params': run.data.params,
                        'artifact_uri': run.info.artifact_uri,
                        'artifact_count': len(artifacts)
                    }
                    
                    print(f"ğŸ“‹ {len(available_models)+1}. Run ID: {run.info.run_id}")
                    print(f"   ğŸ“› ì´ë¦„: {run_info['run_name']}")
                    print(f"   ğŸ“… ì‹œì‘: {run_info['start_time']}")
                    
                    # ì£¼ìš” ë©”íŠ¸ë¦­ í‘œì‹œ
                    if run_info['metrics']:
                        print("   ğŸ“Š ë©”íŠ¸ë¦­:")
                        for key, value in list(run_info['metrics'].items())[:3]:
                            print(f"      â€¢ {key}: {value:.4f}")
                    
                    print(f"   ğŸ“¦ ì•„í‹°íŒ©íŠ¸: {run_info['artifact_count']}ê°œ")
                    available_models.append(run_info)
                    print("-" * 60)
                    
                except Exception as e:
                    print(f"âš ï¸ Run {run.info.run_id[:8]}... ì •ë³´ ì¡°íšŒ ì‹¤íŒ¨: {e}")
                    continue
            
            return available_models
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    def download_model_artifacts_with_retry(self, run_id, local_path="./temp_model", max_retries=3):
        """ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ì´ ìˆëŠ” ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ"""
        for attempt in range(max_retries):
            try:
                print(f"ğŸ“¥ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹œë„ {attempt + 1}/{max_retries}: {run_id}")
                
                # ê¸°ì¡´ ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚­ì œ
                if os.path.exists(local_path):
                    shutil.rmtree(local_path)
                
                # ê°œë³„ ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹œë„
                return self._download_artifacts_individually(run_id, local_path)
                
            except Exception as e:
                print(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹œë„ {attempt + 1} ì‹¤íŒ¨: {e}")
                
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 5
                    print(f"â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    print("âŒ ëª¨ë“  ë‹¤ìš´ë¡œë“œ ì‹œë„ ì‹¤íŒ¨")
                    return None
        
        return None
    
    def _download_artifacts_individually(self, run_id, local_path):
        """ê°œë³„ ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ"""
        try:
            if self.client is None:
                print("âŒ MLflow í´ë¼ì´ì–¸íŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                return None
                
            # ì•„í‹°íŒ©íŠ¸ ëª©ë¡ ì¡°íšŒ
            artifacts = self.client.list_artifacts(run_id)
            if not artifacts:
                print("âŒ ë‹¤ìš´ë¡œë“œí•  ì•„í‹°íŒ©íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return None
            
            os.makedirs(local_path, exist_ok=True)
            
            # ì¤‘ìš”í•œ íŒŒì¼ë“¤ ìš°ì„  ë‹¤ìš´ë¡œë“œ
            priority_files = ['model', 'tokenizer.json', 'config.json']
            downloaded_files = []
            
            print(f"ğŸ” ë°œê²¬ëœ ì•„í‹°íŒ©íŠ¸: {len(artifacts)}ê°œ")
            for artifact in artifacts:
                print(f"   ğŸ“„ {artifact.path}")
            
            # ìš°ì„ ìˆœìœ„ íŒŒì¼ ë‹¤ìš´ë¡œë“œ
            for priority in priority_files:
                for artifact in artifacts:
                    if priority in artifact.path.lower():
                        try:
                            print(f"ğŸ“¥ ìš°ì„  ë‹¤ìš´ë¡œë“œ: {artifact.path}")
                            file_path = self.client.download_artifacts(run_id, artifact.path, local_path)
                            downloaded_files.append(artifact.path)
                            print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {artifact.path}")
                        except Exception as e:
                            print(f"âš ï¸ {artifact.path} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
                            # ì¤‘ìš”í•œ íŒŒì¼ì´ ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰
                            continue
            
            # ë‚˜ë¨¸ì§€ íŒŒì¼ë“¤ ë‹¤ìš´ë¡œë“œ
            for artifact in artifacts:
                if artifact.path not in downloaded_files:
                    try:
                        print(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ: {artifact.path}")
                        file_path = self.client.download_artifacts(run_id, artifact.path, local_path)
                        downloaded_files.append(artifact.path)
                        print(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {artifact.path}")
                    except Exception as e:
                        print(f"âš ï¸ {artifact.path} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ê±´ë„ˆëœ€): {e}")
                        continue
            
            if downloaded_files:
                print(f"âœ… ì´ {len(downloaded_files)}ê°œ íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
                self._inspect_downloaded_artifacts(local_path)
                return local_path
            else:
                print("âŒ ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
                return None
                
        except Exception as e:
            print(f"âŒ ê°œë³„ ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {e}")
            return None
    
    def download_model_artifacts(self, run_id, local_path="./temp_model"):
        """MLflowì—ì„œ ëª¨ë¸ ì•„í‹°íŒ©íŠ¸ ë‹¤ìš´ë¡œë“œ (ê¸°ì¡´ ë©”ì„œë“œ ìœ ì§€)"""
        return self.download_model_artifacts_with_retry(run_id, local_path)
    
    def _inspect_downloaded_artifacts(self, path):
        """ë‹¤ìš´ë¡œë“œëœ ì•„í‹°íŒ©íŠ¸ êµ¬ì¡° ë¶„ì„"""
        print(f"ğŸ” ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ êµ¬ì¡° ë¶„ì„: {path}")
        
        total_size = 0
        file_count = 0
        
        for root, dirs, files in os.walk(path):
            level = root.replace(path, '').count(os.sep)
            indent = ' ' * 2 * level
            print(f"{indent}ğŸ“ {os.path.basename(root)}/")
            
            subindent = ' ' * 2 * (level + 1)
            for file in files:
                file_path = os.path.join(root, file)
                file_size = os.path.getsize(file_path)
                total_size += file_size
                file_count += 1
                print(f"{subindent}ğŸ“„ {file} ({self._format_bytes(file_size)})")
        
        print(f"\nğŸ“Š ì´ {file_count}ê°œ íŒŒì¼, {self._format_bytes(total_size)}")
    
    def _format_bytes(self, bytes):
        """ë°”ì´íŠ¸ë¥¼ ì½ê¸° ì‰¬ìš´ í˜•íƒœë¡œ ë³€í™˜"""
        for unit in ['B', 'KB', 'MB', 'GB']:
            if bytes < 1024.0:
                return f"{bytes:.1f} {unit}"
            bytes /= 1024.0
        return f"{bytes:.1f} TB"
    
    def prepare_model_for_ollama(self, model_path, target_path):
        """Ollama í˜•ì‹ìœ¼ë¡œ ëª¨ë¸ ì¤€ë¹„"""
        try:
            print(f"ğŸ”„ Ollama í˜•ì‹ìœ¼ë¡œ ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
            
            # íƒ€ê²Ÿ ë””ë ‰í† ë¦¬ ìƒì„±
            os.makedirs(target_path, exist_ok=True)
            
            # ëª¨ë¸ íŒŒì¼ ì°¾ê¸° ë° ë³µì‚¬
            model_files = self._find_model_files(model_path)
            
            if not model_files:
                print("âŒ í•„ìš”í•œ ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                print("ğŸ” ë‹¤ìš´ë¡œë“œëœ íŒŒì¼ì„ í™•ì¸í•´ë³´ì„¸ìš”:")
                self._inspect_downloaded_artifacts(model_path)
                return False
            
            # ëª¨ë¸ íƒ€ì… í™•ì¸
            model_type = self._determine_model_type(model_path)
            print(f"ğŸ” ê°ì§€ëœ ëª¨ë¸ íƒ€ì…: {model_type}")
            
            if model_type == "merged_full_model":
                return self._prepare_merged_model(model_path, target_path, model_files)
            elif model_type == "lora_adapter":
                return self._prepare_lora_adapter_model(model_path, target_path, model_files)
            else:
                return self._prepare_regular_model(model_path, target_path, model_files)
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
            return False
    
    def _determine_model_type(self, model_path):
        """ëª¨ë¸ íƒ€ì… ê²°ì •"""
        # config.jsonê³¼ pytorch_model.bin/model.safetensorsê°€ ëª¨ë‘ ìˆìœ¼ë©´ ì™„ì „í•œ ëª¨ë¸
        has_config = False
        has_model_weights = False
        has_adapter = False
        
        for root, dirs, files in os.walk(model_path):
            if 'config.json' in files:
                has_config = True
            if any(f in files for f in ['pytorch_model.bin', 'model.safetensors']):
                has_model_weights = True
            if 'adapter_config.json' in files:
                has_adapter = True
        
        if has_config and has_model_weights:
            return "merged_full_model"
        elif has_adapter:
            return "lora_adapter"
        else:
            return "regular_model"
    
    def _prepare_merged_model(self, model_path, target_path, model_files):
        """ë³‘í•©ëœ ì™„ì „í•œ ëª¨ë¸ ì¤€ë¹„"""
        print("ğŸ”§ ë³‘í•©ëœ ì™„ì „í•œ ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
        
        copied_files = 0
        for src_file, dst_file in model_files.items():
            src_path = os.path.join(model_path, src_file)
            dst_path = os.path.join(target_path, dst_file)
            
            if os.path.exists(src_path):
                os.makedirs(os.path.dirname(dst_path), exist_ok=True)
                shutil.copy2(src_path, dst_path)
                print(f"âœ… ë³µì‚¬ ì™„ë£Œ: {src_file} â†’ {dst_file}")
                copied_files += 1
            else:
                print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {src_file}")
        
        # í•„ìˆ˜ íŒŒì¼ í™•ì¸
        required_files = ['config.json', 'tokenizer.json']
        missing_files = []
        
        for req_file in required_files:
            if not os.path.exists(os.path.join(target_path, req_file)):
                missing_files.append(req_file)
        
        if missing_files:
            print(f"âš ï¸ í•„ìˆ˜ íŒŒì¼ ëˆ„ë½: {missing_files}")
            # ê¸°ë³¸ config.json ìƒì„± ì‹œë„
            if 'config.json' in missing_files:
                print("ğŸ’¡ ê¸°ë³¸ config.jsonì„ ìƒì„±í•©ë‹ˆë‹¤.")
                self._create_basic_config(target_path)
                copied_files += 1
        
        if copied_files > 0:
            print(f"âœ… ì´ {copied_files}ê°œ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ (ë³‘í•©ëœ ëª¨ë¸)")
            return True
        else:
            print("âŒ ë³µì‚¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
    
    def _is_lora_adapter_model(self, model_path):
        """LoRA ì–´ëŒ‘í„° ëª¨ë¸ì¸ì§€ í™•ì¸"""
        # adapter_config.json íŒŒì¼ì´ ìˆëŠ”ì§€ í™•ì¸
        for root, dirs, files in os.walk(model_path):
            if 'adapter_config.json' in files:
                return True
        return False
    
    def _prepare_lora_adapter_model(self, model_path, target_path, model_files):
        """LoRA ì–´ëŒ‘í„° ëª¨ë¸ ì¤€ë¹„"""
        print("ğŸ”§ LoRA ì–´ëŒ‘í„° ëª¨ë¸ ì¤€ë¹„ ì¤‘...")
        
        # ê¸°ë³¸ Gemma ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        base_model_path = "/datasets/github-code/gemma-2b-code-finetuned"
        
        # ê¸°ë³¸ ëª¨ë¸ì´ ìˆëŠ”ì§€ í™•ì¸
        if not os.path.exists(base_model_path):
            print(f"âŒ ê¸°ë³¸ ëª¨ë¸ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {base_model_path}")
            print("ğŸ’¡ ê¸°ë³¸ ëª¨ë¸ ì—†ì´ ì–´ëŒ‘í„°ë§Œìœ¼ë¡œ Ollama ëª¨ë¸ì„ ìƒì„±í•©ë‹ˆë‹¤.")
            return self._create_adapter_only_model(model_path, target_path, model_files)
        
        # ê¸°ë³¸ ëª¨ë¸ íŒŒì¼ ë³µì‚¬
        print(f"ğŸ“¥ ê¸°ë³¸ ëª¨ë¸ ë³µì‚¬ ì¤‘: {base_model_path}")
        base_files_copied = self._copy_base_model_files(base_model_path, target_path)
        
        # ê¸°ë³¸ ëª¨ë¸ íŒŒì¼ì´ ì—†ìœ¼ë©´ ê¸°ë³¸ config.json ìƒì„±
        if base_files_copied == 0:
            print("ğŸ’¡ ê¸°ë³¸ ëª¨ë¸ íŒŒì¼ì´ ì—†ì–´ ê¸°ë³¸ config.jsonì„ ìƒì„±í•©ë‹ˆë‹¤.")
            self._create_basic_config(target_path)
            base_files_copied = 1
        
        # ì–´ëŒ‘í„° íŒŒì¼ ë³µì‚¬
        print("ğŸ“¥ ì–´ëŒ‘í„° íŒŒì¼ ë³µì‚¬ ì¤‘...")
        adapter_files_copied = self._copy_adapter_files(model_path, target_path, model_files)
        
        total_copied = base_files_copied + adapter_files_copied
        
        if total_copied > 0:
            print(f"âœ… ì´ {total_copied}ê°œ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ (ê¸°ë³¸: {base_files_copied}, ì–´ëŒ‘í„°: {adapter_files_copied})")
            return True
        else:
            print("âŒ ë³µì‚¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
    
    def _create_adapter_only_model(self, model_path, target_path, model_files):
        """ì–´ëŒ‘í„°ë§Œìœ¼ë¡œ ëª¨ë¸ ìƒì„±"""
        print("ğŸ”§ ì–´ëŒ‘í„° ì „ìš© ëª¨ë¸ ìƒì„± ì¤‘...")
        
        # ê¸°ë³¸ config.json ìƒì„±
        self._create_basic_config(target_path)
        
        # íŒŒì¼ ë³µì‚¬
        copied_files = 0
        for src_file, dst_file in model_files.items():
            src_path = os.path.join(model_path, src_file)
            dst_path = os.path.join(target_path, dst_file)
            
            if os.path.exists(src_path):
                os.makedirs(os.path.dirname(dst_path), exist_ok=True)
                shutil.copy2(src_path, dst_path)
                print(f"âœ… ë³µì‚¬ ì™„ë£Œ: {src_file} â†’ {dst_file}")
                copied_files += 1
            else:
                print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {src_file}")
        
        if copied_files > 0:
            print(f"âœ… ì´ {copied_files}ê°œ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ")
            return True
        else:
            print("âŒ ë³µì‚¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
    
    def _create_basic_config(self, target_path):
        """ê¸°ë³¸ config.json ìƒì„±"""
        config = {
            "architectures": ["GemmaForCausalLM"],
            "model_type": "gemma",
            "vocab_size": 256000,
            "hidden_size": 2048,
            "intermediate_size": 16384,
            "num_hidden_layers": 18,
            "num_attention_heads": 8,
            "num_key_value_heads": 1,
            "head_dim": 256,
            "max_position_embeddings": 8192,
            "rms_norm_eps": 1e-6,
            "rope_theta": 10000.0,
            "attention_bias": False,
            "attention_dropout": 0.0,
            "hidden_activation": "gelu_pytorch_tanh",
            "hidden_dropout": 0.0,
            "initializer_range": 0.02,
            "pad_token_id": 0,
            "eos_token_id": 1,
            "bos_token_id": 2,
            "tie_word_embeddings": True,
            "torch_dtype": "bfloat16",
            "transformers_version": "4.38.2"
        }
        
        config_path = os.path.join(target_path, "config.json")
        with open(config_path, 'w') as f:
            json.dump(config, f, indent=2)
        
        print(f"âœ… ê¸°ë³¸ config.json ìƒì„±: {config_path}")
    
    def _copy_base_model_files(self, base_model_path, target_path):
        """ê¸°ë³¸ ëª¨ë¸ íŒŒì¼ ë³µì‚¬"""
        copied_files = 0
        base_files = ['config.json', 'pytorch_model.bin', 'model.safetensors']
        
        for file in base_files:
            src_path = os.path.join(base_model_path, file)
            dst_path = os.path.join(target_path, file)
            
            if os.path.exists(src_path):
                shutil.copy2(src_path, dst_path)
                print(f"âœ… ê¸°ë³¸ ëª¨ë¸ ë³µì‚¬: {file}")
                copied_files += 1
            else:
                print(f"âš ï¸ ê¸°ë³¸ ëª¨ë¸ íŒŒì¼ ì—†ìŒ: {file}")
        
        return copied_files
    
    def _copy_adapter_files(self, model_path, target_path, model_files):
        """ì–´ëŒ‘í„° íŒŒì¼ ë³µì‚¬"""
        copied_files = 0
        
        for src_file, dst_file in model_files.items():
            src_path = os.path.join(model_path, src_file)
            dst_path = os.path.join(target_path, dst_file)
            
            if os.path.exists(src_path):
                os.makedirs(os.path.dirname(dst_path), exist_ok=True)
                shutil.copy2(src_path, dst_path)
                print(f"âœ… ì–´ëŒ‘í„° ë³µì‚¬: {src_file} â†’ {dst_file}")
                copied_files += 1
            else:
                print(f"âš ï¸ ì–´ëŒ‘í„° íŒŒì¼ ì—†ìŒ: {src_file}")
        
        return copied_files
    
    def _prepare_regular_model(self, model_path, target_path, model_files):
        """ì¼ë°˜ ëª¨ë¸ ì¤€ë¹„"""
        copied_files = 0
        for src_file, dst_file in model_files.items():
            src_path = os.path.join(model_path, src_file)
            dst_path = os.path.join(target_path, dst_file)
            
            if os.path.exists(src_path):
                # ë””ë ‰í† ë¦¬ ìƒì„±
                os.makedirs(os.path.dirname(dst_path), exist_ok=True)
                shutil.copy2(src_path, dst_path)
                print(f"âœ… ë³µì‚¬ ì™„ë£Œ: {src_file} â†’ {dst_file}")
                copied_files += 1
            else:
                print(f"âš ï¸ íŒŒì¼ ì—†ìŒ: {src_file}")
        
        if copied_files > 0:
            print(f"âœ… ì´ {copied_files}ê°œ íŒŒì¼ ë³µì‚¬ ì™„ë£Œ")
            return True
        else:
            print("âŒ ë³µì‚¬ëœ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
            return False
    
    def _find_model_files(self, model_path):
        """ëª¨ë¸ íŒŒì¼ ìœ„ì¹˜ ì°¾ê¸°"""
        model_files = {}
        
        # ì¼ë°˜ì ì¸ ëª¨ë¸ íŒŒì¼ íŒ¨í„´
        patterns = {
            'config.json': 'config.json',
            'pytorch_model.bin': 'pytorch_model.bin',
            'model.safetensors': 'model.safetensors',
            'tokenizer.json': 'tokenizer.json',
            'tokenizer_config.json': 'tokenizer_config.json',
            'special_tokens_map.json': 'special_tokens_map.json',
            'vocab.json': 'vocab.json',
            'merges.txt': 'merges.txt',
            'generation_config.json': 'generation_config.json'
        }
        
        print(f"ğŸ” ëª¨ë¸ íŒŒì¼ ê²€ìƒ‰ ì¤‘: {model_path}")
        
        # íŒŒì¼ ê²€ìƒ‰
        for root, dirs, files in os.walk(model_path):
            for file in files:
                for pattern, target in patterns.items():
                    if file == pattern:
                        rel_path = os.path.relpath(os.path.join(root, file), model_path)
                        model_files[rel_path] = target
                        print(f"   âœ“ ë°œê²¬: {rel_path}")
        
        # ì¶”ê°€ íŒ¨í„´ ê²€ìƒ‰ (adapter íŒŒì¼ë“¤)
        for root, dirs, files in os.walk(model_path):
            for file in files:
                if file.endswith('.bin') or file.endswith('.safetensors'):
                    if 'adapter' in file.lower() or 'lora' in file.lower():
                        rel_path = os.path.relpath(os.path.join(root, file), model_path)
                        model_files[rel_path] = file
                        print(f"   âœ“ ì–´ëŒ‘í„° íŒŒì¼ ë°œê²¬: {rel_path}")
        
        return model_files
    
    def create_modelfile(self, model_path, model_name, output_path="./Modelfile"):
        """Ollama Modelfile ìƒì„±"""
        try:
            print(f"ğŸ“ Modelfile ìƒì„± ì¤‘: {output_path}")
            
            # ëª¨ë¸ ì •ë³´ ë¡œë“œ
            config_path = os.path.join(model_path, "config.json")
            model_info = {}
            
            if os.path.exists(config_path):
                with open(config_path, 'r') as f:
                    model_info = json.load(f)
                print(f"âœ… ëª¨ë¸ ì„¤ì • ë¡œë“œ: {config_path}")
            
            # Modelfile ë‚´ìš© ìƒì„±
            modelfile_content = f"""FROM {model_path}

TEMPLATE \"\"\"{{{{ if .System }}}}<|im_start|>system
{{{{ .System }}}}<|im_end|>
{{{{ end }}}}{{{{ if .Prompt }}}}<|im_start|>user
{{{{ .Prompt }}}}<|im_end|>
<|im_start|>assistant
{{{{ end }}}}{{{{ .Response }}}}<|im_end|>
\"\"\"

PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER top_k 40
PARAMETER repeat_penalty 1.1
PARAMETER num_ctx 4096
PARAMETER num_predict 512

SYSTEM \"\"\"You are a helpful coding assistant specialized in generating high-quality code snippets. 
You understand multiple programming languages and can generate clean, efficient, and well-documented code.
When generating code, follow these guidelines:
1. Write clean, readable code with proper indentation
2. Include helpful comments when necessary
3. Follow language-specific best practices
4. Generate complete, functional code snippets
\"\"\"
"""
            
            # Modelfile ì €ì¥
            with open(output_path, 'w') as f:
                f.write(modelfile_content)
            
            print(f"âœ… Modelfile ìƒì„± ì™„ë£Œ: {output_path}")
            return True
            
        except Exception as e:
            print(f"âŒ Modelfile ìƒì„± ì‹¤íŒ¨: {e}")
            return False
    
    def register_with_ollama(self, model_name, modelfile_path):
        """Ollamaì— ëª¨ë¸ ë“±ë¡"""
        try:
            print(f"ğŸš€ Ollamaì— ëª¨ë¸ ë“±ë¡ ì¤‘: {model_name}")
            
            # Ollama ì„¤ì¹˜ í™•ì¸
            if not self._check_ollama_installed():
                return False
            
            # ê¸°ì¡´ ëª¨ë¸ ì œê±° (ìˆëŠ” ê²½ìš°)
            self._remove_existing_model(model_name)
            
            # ëª¨ë¸ ìƒì„±
            cmd = ["ollama", "create", model_name, "-f", modelfile_path]
            print(f"ğŸ”§ ì‹¤í–‰ ëª…ë ¹: {' '.join(cmd)}")
            
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            
            print(f"âœ… ëª¨ë¸ ë“±ë¡ ì„±ê³µ!")
            if result.stdout:
                print(f"ì¶œë ¥: {result.stdout}")
            
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")
            if e.stderr:
                print(f"ì—ëŸ¬ ì¶œë ¥: {e.stderr}")
            return False
    
    def _check_ollama_installed(self):
        """Ollama ì„¤ì¹˜ í™•ì¸"""
        try:
            result = subprocess.run(["ollama", "--version"], 
                                  capture_output=True, text=True, check=True)
            print(f"âœ… Ollama ì„¤ì¹˜ë¨: {result.stdout.strip()}")
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            print("âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ì„¤ì¹˜ ë°©ë²•: curl -fsSL https://ollama.ai/install.sh | sh")
            return False
    
    def _remove_existing_model(self, model_name):
        """ê¸°ì¡´ ëª¨ë¸ ì œê±°"""
        try:
            result = subprocess.run(["ollama", "rm", model_name], 
                          capture_output=True, text=True, check=False)
            if result.returncode == 0:
                print(f"ğŸ—‘ï¸ ê¸°ì¡´ ëª¨ë¸ ì œê±°: {model_name}")
            else:
                print(f"ğŸ“ ê¸°ì¡´ ëª¨ë¸ ì—†ìŒ: {model_name}")
        except:
            pass
    
    def test_converted_model(self, model_name, test_prompt="def fibonacci(n):"):
        """ë³€í™˜ëœ ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        try:
            print(f"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘: {model_name}")
            print(f"í”„ë¡¬í”„íŠ¸: {test_prompt}")
            
            cmd = ["ollama", "run", model_name, test_prompt]
            result = subprocess.run(cmd, capture_output=True, text=True, 
                                  check=True, timeout=60)
            
            print("ğŸ“ ìƒì„±ëœ ì‘ë‹µ:")
            print("-" * 50)
            print(result.stdout)
            print("-" * 50)
            
            return True
            
        except subprocess.TimeoutExpired:
            print("â° ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ (60ì´ˆ)")
            return False
        except subprocess.CalledProcessError as e:
            print(f"âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            if e.stderr:
                print(f"ì—ëŸ¬ ì¶œë ¥: {e.stderr}")
            return False
    
    def convert_model(self, run_id, model_name, clean_temp=True):
        """ì „ì²´ ë³€í™˜ í”„ë¡œì„¸ìŠ¤"""
        print(f"ğŸš€ MLflow ëª¨ë¸ì„ Ollamaë¡œ ë³€í™˜ ì‹œì‘")
        print(f"Run ID: {run_id}")
        print(f"ëª¨ë¸ ì´ë¦„: {model_name}")
        print("=" * 60)
        
        temp_dir = None
        try:
            # 0. MLflow ì—°ê²° í…ŒìŠ¤íŠ¸
            if not self.test_mlflow_connection():
                print("âŒ MLflow ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                return False
            
            # 1. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ (ì¬ì‹œë„ ë©”ì»¤ë‹ˆì¦˜ í¬í•¨)
            temp_dir = f"./temp_model_{run_id[:8]}"
            model_path = self.download_model_artifacts_with_retry(run_id, temp_dir)
            if not model_path:
                return False
            
            # 2. ëª¨ë¸ íŒŒì¼ ì¤€ë¹„
            prepared_path = f"./ollama_models/{model_name}"
            if not self.prepare_model_for_ollama(model_path, prepared_path):
                return False
            
            # 3. Modelfile ìƒì„±
            modelfile_path = f"./Modelfile_{model_name}"
            if not self.create_modelfile(prepared_path, model_name, modelfile_path):
                return False
            
            # 4. Ollamaì— ë“±ë¡
            if not self.register_with_ollama(model_name, modelfile_path):
                return False
            
            # 5. í…ŒìŠ¤íŠ¸
            self.test_converted_model(model_name)
            
            print(f"\nğŸ‰ ë³€í™˜ ì™„ë£Œ!")
            print(f"ì‚¬ìš©ë²•: ollama run {model_name}")
            print(f"ëª¨ë¸ íŒŒì¼: {prepared_path}")
            print(f"Modelfile: {modelfile_path}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ë³€í™˜ ì‹¤íŒ¨: {e}")
            return False
        
        finally:
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            if clean_temp and temp_dir and os.path.exists(temp_dir):
                try:
                    shutil.rmtree(temp_dir)
                    print(f"ğŸ§¹ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì™„ë£Œ: {temp_dir}")
                except:
                    print(f"âš ï¸ ì„ì‹œ íŒŒì¼ ì •ë¦¬ ì‹¤íŒ¨: {temp_dir}")

def main():
    parser = argparse.ArgumentParser(description="MLflow ëª¨ë¸ì„ Ollama ëª¨ë¸ë¡œ ë³€í™˜")
    parser.add_argument("--run-id", help="ë³€í™˜í•  MLflow Run ID")
    parser.add_argument("--model-name", help="Ollama ëª¨ë¸ ì´ë¦„")
    parser.add_argument("--experiment", default="Gemma-2b-Code-Finetuning", 
                       help="MLflow ì‹¤í—˜ ì´ë¦„")
    parser.add_argument("--list", action="store_true", help="ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ ëª©ë¡ í‘œì‹œ")
    parser.add_argument("--mlflow-uri", default="http://10.61.3.161:30366/",
                       help="MLflow tracking URI")
    parser.add_argument("--no-cleanup", action="store_true", help="ì„ì‹œ íŒŒì¼ ì •ë¦¬í•˜ì§€ ì•ŠìŒ")
    
    args = parser.parse_args()
    
    # ë³€í™˜ê¸° ì´ˆê¸°í™”
    converter = MLflowToOllamaConverter(args.mlflow_uri)
    
    if args.list:
        # ëª¨ë¸ ëª©ë¡ í‘œì‹œ
        models = converter.list_available_models(args.experiment)
        if models:
            print(f"\nğŸ’¡ ë³€í™˜ ëª…ë ¹ ì˜ˆì‹œ:")
            print(f"python {__file__} --run-id <RUN_ID> --model-name <MODEL_NAME>")
            print(f"python {__file__} --run-id {models[0]['run_id']} --model-name my-gemma-code")
        return
    
    if not args.run_id:
        print("âŒ Run IDê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        print("ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸ì„ ë³´ë ¤ë©´: --list ì˜µì…˜ì„ ì‚¬ìš©í•˜ì„¸ìš”.")
        return
    
    if not args.model_name:
        args.model_name = f"gemma-code-{args.run_id[:8]}"
        print(f"ëª¨ë¸ ì´ë¦„ì´ ì§€ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. ê¸°ë³¸ê°’ ì‚¬ìš©: {args.model_name}")
    
    # ëª¨ë¸ ë³€í™˜ ì‹¤í–‰
    success = converter.convert_model(args.run_id, args.model_name, not args.no_cleanup)
    
    if success:
        print(f"\nâœ… ë³€í™˜ ì„±ê³µ!")
        print(f"ë‹¤ìŒ ëª…ë ¹ìœ¼ë¡œ ëª¨ë¸ì„ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:")
        print(f"ollama run {args.model_name}")
    else:
        print(f"\nâŒ ë³€í™˜ ì‹¤íŒ¨!")
        print(f"ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ --no-cleanup ì˜µì…˜ìœ¼ë¡œ ì¬ì‹œë„í•´ë³´ì„¸ìš”.")
        exit(1)

if __name__ == "__main__":
    main() 