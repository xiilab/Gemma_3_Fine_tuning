#!/usr/bin/env python3
"""
Ollama ì—°ë™ ìŠ¤í¬ë¦½íŠ¸
íŒŒì¸íŠœë‹ëœ Gemma ëª¨ë¸ì„ Ollamaì— ë“±ë¡í•˜ê³  ê´€ë¦¬í•©ë‹ˆë‹¤.
"""

import subprocess
import os
import json
import sys
from pathlib import Path

class OllamaManager:
    def __init__(self, model_name="gemma-code-finetuned"):
        self.model_name = model_name
        self.model_path = "/datasets/github-code/gemma-2b-code-finetuned"
        self.modelfile_path = "./Modelfile"
    
    def check_ollama_installed(self):
        """Ollamaê°€ ì„¤ì¹˜ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸"""
        try:
            result = subprocess.run(["ollama", "--version"], 
                                  capture_output=True, text=True, check=True)
            print(f"âœ… Ollama ì„¤ì¹˜ë¨: {result.stdout.strip()}")
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            print("âŒ Ollamaê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            print("ì„¤ì¹˜ ë°©ë²•: curl -fsSL https://ollama.ai/install.sh | sh")
            return False
    
    def check_model_exists(self):
        """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ì´ ì¡´ì¬í•˜ëŠ”ì§€ í™•ì¸"""
        if os.path.exists(self.model_path):
            print(f"âœ… ëª¨ë¸ íŒŒì¼ ì¡´ì¬: {self.model_path}")
            
            # ì£¼ìš” íŒŒì¼ë“¤ í™•ì¸
            required_files = ["config.json", "pytorch_model.bin", "tokenizer.json"]
            missing_files = []
            
            for file in required_files:
                file_path = os.path.join(self.model_path, file)
                if os.path.exists(file_path):
                    print(f"  âœ“ {file}")
                else:
                    missing_files.append(file)
                    print(f"  âœ— {file} (ëˆ„ë½)")
            
            if missing_files:
                print(f"âš ï¸ ì¼ë¶€ íŒŒì¼ì´ ëˆ„ë½ë˜ì—ˆì§€ë§Œ ì§„í–‰ ê°€ëŠ¥í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.")
            
            return True
        else:
            print(f"âŒ ëª¨ë¸ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.model_path}")
            print("ë¨¼ì € main.pyë¥¼ ì‹¤í–‰í•˜ì—¬ ëª¨ë¸ì„ í•™ìŠµí•´ì£¼ì„¸ìš”.")
            return False
    
    def create_modelfile(self):
        """Modelfile ìƒì„± ë˜ëŠ” ì—…ë°ì´íŠ¸"""
        if os.path.exists(self.modelfile_path):
            print(f"âœ… Modelfile ì´ë¯¸ ì¡´ì¬: {self.modelfile_path}")
            return True
        else:
            print("âŒ Modelfileì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ë¨¼ì € Modelfileì„ ìƒì„±í•´ì£¼ì„¸ìš”.")
            return False
    
    def create_ollama_model(self):
        """Ollamaì— ëª¨ë¸ ë“±ë¡"""
        try:
            print(f"ğŸš€ Ollamaì— ëª¨ë¸ ë“±ë¡ ì¤‘: {self.model_name}")
            
            # ollama create ëª…ë ¹ ì‹¤í–‰
            cmd = ["ollama", "create", self.model_name, "-f", self.modelfile_path]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            
            print(f"âœ… ëª¨ë¸ ë“±ë¡ ì„±ê³µ!")
            print(f"ì¶œë ¥: {result.stdout}")
            
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ ëª¨ë¸ ë“±ë¡ ì‹¤íŒ¨: {e}")
            print(f"ì—ëŸ¬ ì¶œë ¥: {e.stderr}")
            return False
    
    def list_ollama_models(self):
        """ë“±ë¡ëœ Ollama ëª¨ë¸ ëª©ë¡ ì¡°íšŒ"""
        try:
            result = subprocess.run(["ollama", "list"], 
                                  capture_output=True, text=True, check=True)
            print("ğŸ“‹ ë“±ë¡ëœ Ollama ëª¨ë¸ ëª©ë¡:")
            print(result.stdout)
            return True
        except subprocess.CalledProcessError as e:
            print(f"âŒ ëª¨ë¸ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return False
    
    def test_model(self, prompt="def fibonacci(n):"):
        """ëª¨ë¸ í…ŒìŠ¤íŠ¸"""
        try:
            print(f"ğŸ§ª ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì¤‘...")
            print(f"í”„ë¡¬í”„íŠ¸: {prompt}")
            
            # ollama run ëª…ë ¹ ì‹¤í–‰
            cmd = ["ollama", "run", self.model_name, prompt]
            result = subprocess.run(cmd, capture_output=True, text=True, 
                                  check=True, timeout=60)
            
            print("ğŸ“ ìƒì„±ëœ ì‘ë‹µ:")
            print("-" * 50)
            print(result.stdout)
            print("-" * 50)
            
            return True
            
        except subprocess.TimeoutExpired:
            print("â° ì‘ë‹µ ì‹œê°„ ì´ˆê³¼ (60ì´ˆ)")
            return False
        except subprocess.CalledProcessError as e:
            print(f"âŒ ëª¨ë¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
            print(f"ì—ëŸ¬ ì¶œë ¥: {e.stderr}")
            return False
    
    def remove_model(self):
        """ëª¨ë¸ ì œê±°"""
        try:
            print(f"ğŸ—‘ï¸ ëª¨ë¸ ì œê±° ì¤‘: {self.model_name}")
            
            cmd = ["ollama", "rm", self.model_name]
            result = subprocess.run(cmd, capture_output=True, text=True, check=True)
            
            print(f"âœ… ëª¨ë¸ ì œê±° ì™„ë£Œ!")
            return True
            
        except subprocess.CalledProcessError as e:
            print(f"âŒ ëª¨ë¸ ì œê±° ì‹¤íŒ¨: {e}")
            return False
    
    def setup_complete_workflow(self):
        """ì „ì²´ ì„¤ì • ì›Œí¬í”Œë¡œìš°"""
        print("ğŸš€ Ollama ì—°ë™ ì„¤ì • ì‹œì‘")
        print("=" * 50)
        
        # 1. Ollama ì„¤ì¹˜ í™•ì¸
        if not self.check_ollama_installed():
            return False
        
        # 2. ëª¨ë¸ íŒŒì¼ í™•ì¸
        if not self.check_model_exists():
            return False
        
        # 3. Modelfile í™•ì¸
        if not self.create_modelfile():
            return False
        
        # 4. ëª¨ë¸ ë“±ë¡
        if not self.create_ollama_model():
            return False
        
        # 5. ë“±ë¡ í™•ì¸
        self.list_ollama_models()
        
        # 6. í…ŒìŠ¤íŠ¸
        self.test_model()
        
        print("\nğŸ‰ Ollama ì—°ë™ ì„¤ì • ì™„ë£Œ!")
        print(f"ì‚¬ìš©ë²•: ollama run {self.model_name}")
        
        return True

def create_ollama_client_script():
    """Ollama í´ë¼ì´ì–¸íŠ¸ ìŠ¤í¬ë¦½íŠ¸ ìƒì„±"""
    client_script = """#!/usr/bin/env python3
import subprocess
import sys

def chat_with_model(model_name="gemma-code-finetuned"):
    \"\"\"ëŒ€í™”í˜• ëª¨ë“œë¡œ ëª¨ë¸ê³¼ ì±„íŒ…\"\"\"
    print(f"ğŸ¤– {model_name} ëª¨ë¸ê³¼ ì±„íŒ…ì„ ì‹œì‘í•©ë‹ˆë‹¤.")
    print("ì¢…ë£Œí•˜ë ¤ë©´ 'quit' ë˜ëŠ” 'exit'ë¥¼ ì…ë ¥í•˜ì„¸ìš”.")
    print("-" * 50)
    
    while True:
        try:
            prompt = input("\\nğŸ‘¤ You: ").strip()
            
            if prompt.lower() in ['quit', 'exit', 'q']:
                print("ğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
                break
            
            if not prompt:
                continue
            
            print("ğŸ¤– Assistant: ", end="", flush=True)
            
            # ollama run ëª…ë ¹ìœ¼ë¡œ ì‹¤ì‹œê°„ ì‘ë‹µ
            process = subprocess.Popen(
                ["ollama", "run", model_name, prompt],
                stdout=subprocess.PIPE,
                stderr=subprocess.PIPE,
                text=True,
                bufsize=1,
                universal_newlines=True
            )
            
            # ì‹¤ì‹œê°„ ì¶œë ¥
            for line in process.stdout:
                print(line, end="", flush=True)
            
            process.wait()
            
        except KeyboardInterrupt:
            print("\\nğŸ‘‹ ì±„íŒ…ì„ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        except Exception as e:
            print(f"\\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")

if __name__ == "__main__":
    if len(sys.argv) > 1:
        model_name = sys.argv[1]
        chat_with_model(model_name)
    else:
        chat_with_model()
"""
    
    with open("ollama_chat.py", "w", encoding="utf-8") as f:
        f.write(client_script)
    
    # ì‹¤í–‰ ê¶Œí•œ ë¶€ì—¬
    os.chmod("ollama_chat.py", 0o755)
    print("âœ… Ollama ì±„íŒ… í´ë¼ì´ì–¸íŠ¸ ìƒì„±: ollama_chat.py")

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("ğŸ¦™ Ollama ì—°ë™ ê´€ë¦¬ì")
    print("=" * 40)
    
    manager = OllamaManager()
    
    if len(sys.argv) > 1:
        command = sys.argv[1].lower()
        
        if command == "setup":
            manager.setup_complete_workflow()
        elif command == "list":
            manager.list_ollama_models()
        elif command == "test":
            prompt = sys.argv[2] if len(sys.argv) > 2 else "def fibonacci(n):"
            manager.test_model(prompt)
        elif command == "remove":
            manager.remove_model()
        elif command == "chat":
            create_ollama_client_script()
        else:
            print(f"âŒ ì•Œ ìˆ˜ ì—†ëŠ” ëª…ë ¹: {command}")
            print_usage()
    else:
        # ëŒ€í™”í˜• ëª¨ë“œ
        interactive_mode(manager)

def print_usage():
    """ì‚¬ìš©ë²• ì¶œë ¥"""
    print("""
ì‚¬ìš©ë²•:
  python ollama_setup.py setup    # ì „ì²´ ì„¤ì • ì‹¤í–‰
  python ollama_setup.py list     # ëª¨ë¸ ëª©ë¡ ì¡°íšŒ
  python ollama_setup.py test     # ëª¨ë¸ í…ŒìŠ¤íŠ¸
  python ollama_setup.py remove   # ëª¨ë¸ ì œê±°
  python ollama_setup.py chat     # ì±„íŒ… í´ë¼ì´ì–¸íŠ¸ ìƒì„±
  python ollama_setup.py          # ëŒ€í™”í˜• ëª¨ë“œ
""")

def interactive_mode(manager):
    """ëŒ€í™”í˜• ëª¨ë“œ"""
    while True:
        print("\nğŸ“‹ ì‚¬ìš© ê°€ëŠ¥í•œ ëª…ë ¹:")
        print("1. ì „ì²´ ì„¤ì • ì‹¤í–‰")
        print("2. ëª¨ë¸ ëª©ë¡ ì¡°íšŒ")
        print("3. ëª¨ë¸ í…ŒìŠ¤íŠ¸")
        print("4. ëª¨ë¸ ì œê±°")
        print("5. ì±„íŒ… í´ë¼ì´ì–¸íŠ¸ ìƒì„±")
        print("0. ì¢…ë£Œ")
        
        choice = input("\nì„ íƒí•˜ì„¸ìš” (0-5): ").strip()
        
        if choice == "0":
            print("ğŸ‘‹ ì¢…ë£Œí•©ë‹ˆë‹¤.")
            break
        elif choice == "1":
            manager.setup_complete_workflow()
        elif choice == "2":
            manager.list_ollama_models()
        elif choice == "3":
            prompt = input("í…ŒìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ì…ë ¥ (ê¸°ë³¸ê°’: def fibonacci(n):): ").strip()
            if not prompt:
                prompt = "def fibonacci(n):"
            manager.test_model(prompt)
        elif choice == "4":
            confirm = input(f"ì •ë§ë¡œ '{manager.model_name}' ëª¨ë¸ì„ ì œê±°í•˜ì‹œê² ìŠµë‹ˆê¹Œ? (y/N): ").strip().lower()
            if confirm == 'y':
                manager.remove_model()
        elif choice == "5":
            create_ollama_client_script()
        else:
            print("âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤.")

if __name__ == "__main__":
    main() 