#!/usr/bin/env python3
"""
MLflow ì—…ë¡œë“œ ì¬ì‹œë„ ìŠ¤í¬ë¦½íŠ¸
í° íŒŒì¼ ì—…ë¡œë“œ ì‹œ ë°œìƒí•˜ëŠ” timeout ì˜¤ë¥˜ë¥¼ í•´ê²°í•˜ê¸° ìœ„í•œ ìŠ¤í¬ë¦½íŠ¸
"""

import os
import time
import requests
import mlflow
import mlflow.pytorch
from mlflow import MlflowClient
from datetime import datetime
import shutil
import tempfile
from pathlib import Path
import json

# MLflow ì„¤ì •
mlflow.set_tracking_uri("http://10.61.3.161:30744/")
experiment_name = "Gemma-2b-Code-Finetuning"
mlflow.set_experiment(experiment_name)

def check_mlflow_server():
    """MLflow ì„œë²„ ìƒíƒœ í™•ì¸"""
    try:
        response = requests.get("http://10.61.3.161:30744/health", timeout=10)
        if response.status_code == 200:
            print("âœ… MLflow ì„œë²„ê°€ ì •ìƒ ì‘ë™ ì¤‘ì…ë‹ˆë‹¤.")
            return True
        else:
            print(f"âŒ MLflow ì„œë²„ ì‘ë‹µ ì˜¤ë¥˜: {response.status_code}")
            return False
    except Exception as e:
        print(f"âŒ MLflow ì„œë²„ ì—°ê²° ì‹¤íŒ¨: {e}")
        return False

def upload_large_files_chunked(model_path, run_id, max_file_size_mb=100):
    """
    í° íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì—…ë¡œë“œí•˜ëŠ” í•¨ìˆ˜
    """
    client = MlflowClient()
    model_path = Path(model_path)
    
    print(f"ğŸ“¤ ëª¨ë¸ íŒŒì¼ì„ ì²­í¬ ë‹¨ìœ„ë¡œ ì—…ë¡œë“œ ì¤‘: {model_path}")
    
    # íŒŒì¼ í¬ê¸°ë³„ë¡œ ë¶„ë¥˜
    small_files = []
    large_files = []
    
    for file_path in model_path.rglob("*"):
        if file_path.is_file():
            size_mb = file_path.stat().st_size / (1024 * 1024)
            if size_mb > max_file_size_mb:
                large_files.append((file_path, size_mb))
            else:
                small_files.append((file_path, size_mb))
    
    print(f"ğŸ“Š íŒŒì¼ ë¶„ì„ ì™„ë£Œ:")
    print(f"   - ì‘ì€ íŒŒì¼: {len(small_files)}ê°œ")
    print(f"   - í° íŒŒì¼: {len(large_files)}ê°œ")
    
    # 1. ì‘ì€ íŒŒì¼ë“¤ ë¨¼ì € ì—…ë¡œë“œ
    print("ğŸ“¤ ì‘ì€ íŒŒì¼ë“¤ ì—…ë¡œë“œ ì¤‘...")
    for file_path, size_mb in small_files:
        try:
            relative_path = file_path.relative_to(model_path)
            artifact_path = f"model/{relative_path}"
            
            print(f"   ì—…ë¡œë“œ ì¤‘: {relative_path} ({size_mb:.1f}MB)")
            client.log_artifact(run_id, str(file_path), artifact_path.parent)
            time.sleep(0.5)  # ì„œë²„ ë¶€í•˜ ë°©ì§€
            
        except Exception as e:
            print(f"   âŒ íŒŒì¼ ì—…ë¡œë“œ ì‹¤íŒ¨: {relative_path} - {e}")
    
    # 2. í° íŒŒì¼ë“¤ ê°œë³„ ì—…ë¡œë“œ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    print("ğŸ“¤ í° íŒŒì¼ë“¤ ì—…ë¡œë“œ ì¤‘...")
    for file_path, size_mb in large_files:
        relative_path = file_path.relative_to(model_path)
        artifact_path = f"model/{relative_path}"
        
        print(f"   ì—…ë¡œë“œ ì¤‘: {relative_path} ({size_mb:.1f}MB)")
        
        # ì¬ì‹œë„ ë¡œì§
        max_retries = 3
        for attempt in range(max_retries):
            try:
                print(f"      ì‹œë„ {attempt + 1}/{max_retries}")
                client.log_artifact(run_id, str(file_path), artifact_path.parent)
                print(f"      âœ… ì„±ê³µ")
                break
                
            except Exception as e:
                print(f"      âŒ ì‹¤íŒ¨: {e}")
                if attempt < max_retries - 1:
                    wait_time = (attempt + 1) * 10
                    print(f"      â³ {wait_time}ì´ˆ í›„ ì¬ì‹œë„...")
                    time.sleep(wait_time)
                else:
                    print(f"      âŒ ìµœì¢… ì‹¤íŒ¨: {relative_path}")
        
        time.sleep(2)  # í° íŒŒì¼ ê°„ ëŒ€ê¸° ì‹œê°„

def create_model_registry_entry(run_id, model_name, model_path):
    """
    ëª¨ë¸ì„ Model Registryì— ë“±ë¡
    """
    try:
        client = MlflowClient()
        
        # ëª¨ë¸ URI ìƒì„±
        model_uri = f"runs:/{run_id}/model"
        
        # ëª¨ë¸ ë“±ë¡
        print(f"ğŸ“ Model Registryì— '{model_name}' ë“±ë¡ ì¤‘...")
        model_version = mlflow.register_model(
            model_uri=model_uri,
            name=model_name,
            description=f"Gemma-2b fine-tuned model (uploaded at {datetime.now()})"
        )
        
        print(f"âœ… ëª¨ë¸ì´ ì„±ê³µì ìœ¼ë¡œ ë“±ë¡ë˜ì—ˆìŠµë‹ˆë‹¤: {model_name} v{model_version.version}")
        return model_version
        
    except Exception as e:
        print(f"âŒ Model Registry ë“±ë¡ ì‹¤íŒ¨: {e}")
        return None

def upload_model_with_retry(model_path, model_name="gemma-2b-code-finetuned", max_retries=3):
    """
    ëª¨ë¸ ì—…ë¡œë“œ ë©”ì¸ í•¨ìˆ˜ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
    """
    if not check_mlflow_server():
        print("âŒ MLflow ì„œë²„ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return False
    
    model_path = Path(model_path)
    if not model_path.exists():
        print(f"âŒ ëª¨ë¸ ê²½ë¡œê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {model_path}")
        return False
    
    print(f"ğŸš€ ëª¨ë¸ ì—…ë¡œë“œ ì‹œì‘: {model_path}")
    
    # ìƒˆë¡œìš´ MLflow run ì‹œì‘
    with mlflow.start_run(run_name=f"model-upload-{datetime.now().strftime('%Y%m%d_%H%M%S')}"):
        run_id = mlflow.active_run().info.run_id
        print(f"ğŸ“‹ MLflow Run ID: {run_id}")
        
        # ëª¨ë¸ ì •ë³´ ë¡œê¹…
        model_info = {
            "model_name": model_name,
            "model_path": str(model_path),
            "upload_time": datetime.now().isoformat(),
            "model_type": "merged_full_model" if "merged" in str(model_path) else "lora_adapter",
            "ollama_compatible": "merged" in str(model_path)
        }
        
        mlflow.log_params(model_info)
        
        # ëª¨ë¸ íŒŒì¼ í¬ê¸° í™•ì¸
        total_size = sum(f.stat().st_size for f in model_path.rglob("*") if f.is_file())
        total_size_mb = total_size / (1024 * 1024)
        print(f"ğŸ“Š ì´ ëª¨ë¸ í¬ê¸°: {total_size_mb:.1f}MB")
        
        mlflow.log_metric("model_size_mb", total_size_mb)
        
        # ì²­í¬ ì—…ë¡œë“œ ì‹œë„
        try:
            upload_large_files_chunked(model_path, run_id)
            
            # ëª¨ë¸ ì •ë³´ íŒŒì¼ ìƒì„±
            info_file = "upload_info.json"
            with open(info_file, "w") as f:
                json.dump(model_info, f, indent=2)
            
            mlflow.log_artifact(info_file)
            os.remove(info_file)
            
            # Model Registryì— ë“±ë¡
            model_version = create_model_registry_entry(run_id, model_name, model_path)
            
            if model_version:
                print(f"ğŸ‰ ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ!")
                print(f"   - Run ID: {run_id}")
                print(f"   - Model Name: {model_name}")
                print(f"   - Version: {model_version.version}")
                print(f"   - MLflow UI: http://10.61.3.161:30744/#/experiments")
                return True
            else:
                print("âš ï¸ íŒŒì¼ ì—…ë¡œë“œëŠ” ì„±ê³µí–ˆì§€ë§Œ Model Registry ë“±ë¡ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
                return False
                
        except Exception as e:
            print(f"âŒ ì—…ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return False

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    print("=== MLflow ëª¨ë¸ ì—…ë¡œë“œ ì¬ì‹œë„ ìŠ¤í¬ë¦½íŠ¸ ===")
    
    # ê¸°ë³¸ ê²½ë¡œë“¤
    merged_model_path = "/datasets/github-code/gemma-2b-code-finetuned_merged"
    lora_model_path = "/datasets/github-code/gemma-2b-code-finetuned"
    
    # 1. ë³‘í•©ëœ ëª¨ë¸ ì—…ë¡œë“œ ì‹œë„
    if os.path.exists(merged_model_path):
        print("\nğŸ¯ ë³‘í•©ëœ ëª¨ë¸ (Ollama í˜¸í™˜) ì—…ë¡œë“œ ì‹œë„...")
        success = upload_model_with_retry(
            merged_model_path, 
            "gemma-2b-code-finetuned-merged"
        )
        
        if success:
            print("âœ… ë³‘í•©ëœ ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ!")
        else:
            print("âŒ ë³‘í•©ëœ ëª¨ë¸ ì—…ë¡œë“œ ì‹¤íŒ¨")
    
    # 2. LoRA ëª¨ë¸ ì—…ë¡œë“œ ì‹œë„
    if os.path.exists(lora_model_path):
        print("\nğŸ¯ LoRA ì–´ëŒ‘í„° ëª¨ë¸ ì—…ë¡œë“œ ì‹œë„...")
        success = upload_model_with_retry(
            lora_model_path, 
            "gemma-2b-code-finetuned-lora"
        )
        
        if success:
            print("âœ… LoRA ëª¨ë¸ ì—…ë¡œë“œ ì™„ë£Œ!")
        else:
            print("âŒ LoRA ëª¨ë¸ ì—…ë¡œë“œ ì‹¤íŒ¨")
    
    print("\n=== ì—…ë¡œë“œ ìŠ¤í¬ë¦½íŠ¸ ì™„ë£Œ ===")

if __name__ == "__main__":
    main() 